{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO342vlU21Ym8203s/cOzU+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harnalashok/CatEncodersFamily/blob/main/avazu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "17th May, 2023\n",
        "17th May, 2023\n",
        "Objective: Avazu:\n",
        "    Classification\n",
        "    And\n",
        "    SMOTE\n",
        "    with less data\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "MCpjCIXdS2z1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%reset -f\n",
        "\n",
        "# 1.0 Call libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys    # For pointig to folder where our module is\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 1.01\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import xgboost as xgb\n",
        "#import umap  # Takes long time to import\n",
        "\n",
        "# 1.02 Misc\n",
        "import gc , os,time"
      ],
      "metadata": {
        "id": "ZpOTFyCjS7bO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Fl77A48MSJN"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# 1.05\n",
        "#dataPath =                 \"C:\\\\Users\\\\Ashok\\\\OneDrive\\\\Documents\\\\talkingdata\\\\29042023_avazu\\\\\"\n",
        "#modulePath = dataPath\n",
        "#modelsPath =               \"C:\\\\Users\\\\Ashok\\\\OneDrive\\\\Documents\\\\talkingdata\\\\29042023_avazu\\\\allmodels\\\\models\\\\\"\n",
        "#pathToStoreProgress =      \"C:\\\\Users\\\\Ashok\\\\OneDrive\\\\Documents\\\\talkingdata\\\\29042023_avazu\\\\allmodels\\\\progress\\\\\"\n",
        "#master =  dataPath + \"master\\\\\"\n",
        "\n",
        "dataPath =                 \"D:\\\\avazu\\\\\"\n",
        "modulePath = dataPath\n",
        "modelsPath =               \"D:\\\\avazu\\\\allmodels\\\\models\\\\\"\n",
        "pathToStoreProgress =      \"D:\\\\avazu\\\\allmodels\\\\progress\\\\\"\n",
        "master =  dataPath + \"master\\\\\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 1.03 Home made modules\n",
        "sys.path.append(modulePath)\n",
        "import utils\n",
        "# DO NOT CALL, as:\n",
        "#  from scikitlearnclass import CatEncoders\n",
        "#   Subsequently saving in pickle gives error.\n",
        "import scikitlearnclass\n",
        "\n",
        "# 1.04\n",
        "#import importlib; importlib.reload(network_features)\n",
        "#import importlib; importlib.reload(utils)\n",
        "#import importlib; importlib.reload(scikitlearnclass)\n",
        "\n",
        "\n",
        "# 2.0 Decide program-wide\n",
        "rng = np.random.RandomState(0)\n",
        "\n",
        "\n",
        "\n",
        "## Read Data\n",
        "os.chdir(dataPath)\n",
        "\n",
        "# Our field datatypes:\n",
        "dtypes = {\n",
        "        'C1'              : 'uint16',\n",
        "        'banner_pos'      : 'uint8',\n",
        "        'device_type'     : 'uint8',\n",
        "        'device_conn_type': 'uint8',\n",
        "        'C14'             : 'uint16',\n",
        "        'C15'             : 'uint16',\n",
        "        'C16'             : 'uint16',\n",
        "        'C17'             : 'uint16',\n",
        "        'C18'             : 'uint8',\n",
        "        'C19'             : 'uint16',\n",
        "        'C20'             : 'int32',\n",
        "        'click'           : 'uint8'\n",
        "        }\n",
        "\n",
        "print('load train...')\n",
        "\n",
        "\n",
        "# 3.1 Read a fraction of data\n",
        "total_lines = 40428967   #  (40428967, 24)\n",
        "read_lines =   3000000    # 7.4%  Reduce it if less RAM\n",
        "\n",
        "# 3.2 Read randomly 'p' fraction of files\n",
        "#     Ref: https://stackoverflow.com/a/48589768\n",
        "\n",
        "p = read_lines/total_lines  # fraction of lines to read\n",
        "\n",
        "# 3.2.1 How to pick up random rows from hard-disk\n",
        "#       without first loading the complete file in RAM\n",
        "#       Toss a coin:\n",
        "#           At each row, toss a biased-coin: 60%->Head, 40%->tail\n",
        "#           If tail comes, select the row else not.\n",
        "#           Toss a coin: random.random()\n",
        "#           Head occurs if value > 0.6 else it is tail\n",
        "#\n",
        "#       We do not toss the coin for header row. Keep the header\n",
        "# From https://www.kaggle.com/code/gauravduttakiit/data-sampling\n",
        "parse_date = lambda val : pd.datetime.strptime(val, '%y%m%d%H')\n",
        "train = pd.read_csv(\n",
        "                     dataPath + \"train.gz\",  # Not reading test.csv.zip\n",
        "                     header=0,  # First row is header-row\n",
        "                                # 'and' operator returns True if both values are True\n",
        "                                #  random.random() returns values between (0,1)\n",
        "                                #  No of rows skipped will be around 60% of total\n",
        "                      skiprows=lambda i: (i >0 ) and (np.random.random() > p),    # (i>0) implies skip first header row\n",
        "                      dtype=dtypes,\n",
        "                      parse_dates=['hour'],\n",
        "                      date_parser=parse_date\n",
        "                      # We read all columns. Here are the column-names\n",
        "                      #  in the sequence they occur in the train data\n",
        "                      #usecols=['ip','app','device','os', 'channel', 'click_time', 'attributed_time', 'is_attributed']\n",
        "                    )   # Takes 3 minute\n",
        "\n",
        "\n",
        "# Explore read data:\n",
        "train.shape     # (3000756, 24)\n",
        "train.head(3)\n",
        "train.dtypes\n",
        "\"\"\"\n",
        "id                          uint64\n",
        "click                        uint8\n",
        "hour                datetime64[ns]\n",
        "C1                          uint16\n",
        "banner_pos                   uint8\n",
        "site_id                     object\n",
        "site_domain                 object\n",
        "site_category               object\n",
        "app_id                      object\n",
        "app_domain                  object\n",
        "app_category                object\n",
        "device_id                   object\n",
        "device_ip                   object\n",
        "device_model                object\n",
        "device_type                  uint8\n",
        "device_conn_type             uint8\n",
        "C14                         uint16\n",
        "C15                         uint16\n",
        "C16                         uint16\n",
        "C17                         uint16\n",
        "C18                          uint8\n",
        "C19                         uint16\n",
        "C20                          int32\n",
        "C21                          int64\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Separate target, and drop id\n",
        "y = train.pop(\"click\")\n",
        "train.pop(\"id\")\n",
        "\n",
        "# Our list of columns:\n",
        "train.columns\n",
        "\n",
        "\n",
        "\n",
        "# Here is a list of all columns except 'hour'\n",
        "cols = [\"C1\",\"banner_pos\",\"site_id\",\"site_domain\",\"site_category\",\n",
        "        \"app_id\",\"app_domain\",\"app_category\",\"device_id\",\"device_ip\",\n",
        "        \"device_model\", \"device_type\", \"device_conn_type\",\t\"C14\",\n",
        "        \"C15\", \"C16\",\t\"C17\",\t\"C18\",\t\"C19\",\t\"C20\",\t\"C21\"]\n",
        "\n",
        "\n",
        "\n",
        "# Check no of unique values\n",
        "# Columns [device_id, device_ip]\n",
        "# Have very large no of unique values\n",
        "for i in cols:\n",
        "    print(i,\"\\t\",train[i].nunique())\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Data: 3000000 rows\n",
        "C1 \t 7\n",
        "banner_pos \t 7\n",
        "site_id \t 3256\n",
        "site_domain \t 3924\n",
        "site_category \t 22\n",
        "app_id \t 4434\n",
        "app_domain \t 277\n",
        "app_category \t 27\n",
        "device_id \t 394043    <==v large\n",
        "device_ip \t 1313203   <==v large\n",
        "device_model \t 6122\n",
        "device_type \t 5\n",
        "device_conn_type \t 4\n",
        "C14 \t 2428\n",
        "C15 \t 8\n",
        "C16 \t 9\n",
        "C17 \t 426\n",
        "C18 \t 4\n",
        "C19 \t 67\n",
        "C20 \t 166\n",
        "C21 \t 60\n",
        "-----------\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Rename columns requirement of our transformer class\n",
        "# Column names are not to have digits\n",
        "cols = {\n",
        "        \"C1\" : \"Cone\",\n",
        "        \"banner_pos\" : \"bannerpos\",\n",
        "        \"site_id\"    :  \"siteid\",\n",
        "        \"site_domain\": \"sitedomain\",\n",
        "        \"site_category\" : \"sitecategory\",\n",
        "        \"app_id\"        : \"appid\",\n",
        "        \"app_domain\"    : \"appdomain\",\n",
        "        \"app_category\"  : \"appcategory\",\n",
        "        \"device_model\"  : \"devicemodel\",\n",
        "        \"device_type\"   : \"devicetype\",\n",
        "        \"device_conn_type\" : \"deviceconntype\",\n",
        "        \"C14\" : \"Cfourteen\",\n",
        "        \"C15\" : \"Cfifteen\",\n",
        "        \"C16\" : \"Csixteen\",\n",
        "        \"C17\" : \"Cseventeen\",\n",
        "        \"C18\" : \"Ceighteen\",\n",
        "        \"C19\" : \"Cnineteen\",\n",
        "        \"C20\" : \"Ctwenty\",\n",
        "        \"C21\" : \"Ctwentyone\"\n",
        "        }\n",
        "\n",
        "train = train.rename( columns = cols)\n",
        "train.shape  #    (5999306, 22)\n",
        "train.columns\n",
        "train.head(3)\n",
        "\n",
        "\n",
        "# Extract date components then remove 'hour' column\n",
        "train['month'] = train['hour'].dt.month\n",
        "train['dayofweek'] = train['hour'].dt.dayofweek\n",
        "train['day'] = train['hour'].dt.day\n",
        "train['hour_time'] = train['hour'].dt.hour\n",
        "train.pop('hour')\n",
        "train.head(3)\n",
        "\n",
        "# Save train data\n",
        "os.chdir(master)\n",
        "train.to_pickle(\"train.pkl\")\n",
        "y.to_pickle(\"y.pkl\")\n",
        "\n",
        "####\n",
        "# Read saved data\n",
        "os.chdir(master)\n",
        "train = pd.read_pickle(\"train.pkl\")\n",
        "y = pd.read_pickle(\"y.pkl\")\n",
        "####\n",
        "\n",
        "# Some columns need label encoding for feeding\n",
        "#  into xgboost classifier. Let us check.\n",
        "train.iloc[:, :5].head()   # siteid, sitedomain,sitecategory\n",
        "train.iloc[:, 5:10].head()   # appid,appdomain,appcategory,device_id,device_ip\n",
        "train.iloc[:, 10:15].head()  # devicemodel\n",
        "\n",
        "ColumnsToEncode = [\"siteid\", \"sitedomain\", \"appid\", \"appdomain\",\n",
        "                   \"appcategory\", \"device_id\", \"device_ip\",\n",
        "                   \"devicemodel\", 'sitecategory']\n",
        "\n",
        "\n",
        "# Check our list once again:\n",
        "train[ColumnsToEncode[:4]].head()\n",
        "train[ColumnsToEncode[4:]].head()\n",
        "\n",
        "\n",
        "\n",
        "# Label encode some columns of train data:\n",
        "#  Takes time:\n",
        "\n",
        "dict_ = {}  # Save label encoder objects here\n",
        "for i in ColumnsToEncode:\n",
        "    le = LabelEncoder()\n",
        "    le.fit(train[i])\n",
        "    train[i] = le.transform(train[i])\n",
        "    dict_[i] = le\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Check again if encoding done?\n",
        "train[ColumnsToEncode[:4]].head()\n",
        "train[ColumnsToEncode[4:]].head()\n",
        "\n",
        "# Save dict of LabelEncoder objects for future use\n",
        "utils.savePythonObject(dict_, \"labelEnoders.pkl\", master)\n",
        "# Can restore dict of LabelEncoder objects, as:\n",
        "le_objs = utils.restorePythonObject(\"labelEnoders.pkl\", master)\n",
        "le_objs\n",
        "\n",
        "\n",
        "# Save label encoded train data\n",
        "os.chdir(master)\n",
        "train.to_pickle(\"train_encoded.pkl\")\n",
        "y.to_pickle(\"y.pkl\")\n",
        "\n",
        "####\n",
        "# Read label encoded saved data\n",
        "os.chdir(master)\n",
        "train = pd.read_pickle(\"train_encoded.pkl\")\n",
        "y = pd.read_pickle(\"y.pkl\")\n",
        "####\n",
        "# Check, if read\n",
        "train.head()\n",
        "\n",
        "######################\n",
        "######################\n",
        "\n",
        "# Split our data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "                                                    train,\n",
        "                                                    y,\n",
        "                                                    test_size = 0.25,\n",
        "                                                    stratify= y,\n",
        "                                                    random_state= rng)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Check shapes:\n",
        "X_train.shape    # (2250567, 25)\n",
        "X_test.shape     # (750189, 25)\n",
        "y_train.shape    # (2250567,)\n",
        "y_test.shape     # (750189,)\n",
        "\n",
        "\n",
        "# As data is large, save split data:\n",
        "os.chdir(master)\n",
        "X_train.to_pickle(\"X_train.pkl\")\n",
        "X_test.to_pickle(\"X_test.pkl\")\n",
        "y_train.to_pickle(\"y_train.pkl\")\n",
        "y_test.to_pickle(\"y_test.pkl\")\n",
        "\n",
        "\n",
        "####\n",
        "# Read split data\n",
        "os.chdir(master)\n",
        "X_train = pd.read_pickle(\"X_train.pkl\")\n",
        "X_test = pd.read_pickle(\"X_test.pkl\")\n",
        "y_train = pd.read_pickle(\"y_train.pkl\")\n",
        "y_test = pd.read_pickle(\"y_test.pkl\")\n",
        "#####\n",
        "\n",
        "\n",
        "# Recheck shapes:\n",
        "X_train.shape    # (2250567, 25)\n",
        "X_test.shape     # (750189, 25)\n",
        "y_train.shape    # (2250567,)\n",
        "y_test.shape     # (750189,)\n",
        "\n",
        "\n",
        "\n",
        "# Check distribution of levels in split data\n",
        "y_test.value_counts(normalize = True)  # 83%:17%\n",
        "y_train.value_counts(normalize = True) # 83%:17%\n",
        "\n",
        "\n",
        "# Check nulls. None.\n",
        "X_train.isnull().sum()\n",
        "X_test.isnull().sum()\n",
        "\n",
        "\n",
        "##*********************************\n",
        "## Developing models for transformation:\n",
        "##*********************************\n",
        "\n",
        "#  Which are our cat columns\n",
        "\n",
        "\n",
        "# We cosider:\n",
        "cat_cols = ['Cone', 'bannerpos', 'siteid', 'sitedomain', 'sitecategory',\n",
        "            'appid', 'appdomain', 'appcategory', 'devicemodel', 'devicetype',\n",
        "            'deviceconntype', 'Cfourteen', 'Cfifteen','Csixteen', 'Cseventeen',\n",
        "            'Ceighteen', 'Cnineteen', 'Ctwenty',\n",
        "            'Ctwentyone']\n",
        "\n",
        "len(cat_cols)  # 19\n",
        "\n",
        "# Remaining columns are a mix of numeric and cat:\n",
        "# device_id and device_ip have very large number of levels\n",
        "# So we have ignored them in our above list.\n",
        "rem_cols =  set(train.columns).difference(set(cat_cols))\n",
        "rem_cols\n",
        "\"\"\"\n",
        "{'day', 'dayofweek', 'device_id', 'device_ip', 'hour_time', 'month'}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# As number of cat_cols are quite large\n",
        "#  Keep interacting columns as null\n",
        "interactingCatCols = []\n",
        "\n",
        "# Instantiate CustomTransformer class:\n",
        "# WARNING: If you do not call the class as:\n",
        "#          scikitlearnclass.CustomTransformer,\n",
        "#          pickle does not save the class-object\n",
        "ct = scikitlearnclass.CatEncoder(pathToStoreProgress, # Progress file is stored here\n",
        "                                 modelsPath,          # Graph files will be saved here\n",
        "                                 cMeasures=  [ 1,1,1,0,None,0,0],\n",
        "                                 subseqlength = 2, # It is the default\n",
        "                                 n_iter =1,  # It is the default\n",
        "                                 k = 40,  # Irrelevalent here\n",
        "                                          #  as we are not calculating betweenness centrality\n",
        "                                 saveGraph = True\n",
        "                       )\n",
        "\n",
        "# Fit it on X_train:\n",
        "gc.collect()\n",
        "start = time.time()\n",
        "ct.fit(X_train, cat_cols, interactingCatCols)\n",
        "end = time.time()\n",
        "print((end-start)/60)    # 88 minutes(6000000),\n",
        "\n",
        "\n",
        "# Save fitted class object for later use:\n",
        "utils.savePythonObject(ct, \"transformer.pkl\", modelsPath)\n",
        "\n",
        "# We delete existing class object\n",
        "del ct\n",
        "\n",
        "# Read back saved class object:\n",
        "ct = utils.restorePythonObject(\"transformer.pkl\", modelsPath)\n",
        "ct\n",
        "\n",
        "# Transform X_train now:\n",
        "gc.collect()\n",
        "start = time.time()\n",
        "out_tr = ct.transform(X_train[cat_cols])\n",
        "end = time.time()     # 38 minutes (6000000)\n",
        "print((end -start)/60)\n",
        "\n",
        "\n",
        "# Transform test data\n",
        "start = time.time()\n",
        "out_te = ct.transform(X_test[cat_cols])\n",
        "end = time.time()\n",
        "print((end -start)/60)    # 14 min\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "# Check shapes:\n",
        "out_te.shape      #    (750189, 599)\n",
        "out_te.columns    #   Includes original columns also\n",
        "out_tr.shape      #  (2250567, 599)\n",
        "\n",
        "# Remove low variance columns\n",
        "# out_te = utils.removeLowVarCols( out_te , pca = False)\n",
        "\n",
        "# Save transformed data:\n",
        "os.chdir(master)\n",
        "out_te.to_pickle(\"X_test_transformed.pkl\")\n",
        "y_test.to_pickle(\"y_test.pkl\")\n",
        "\n",
        "\n",
        "os.chdir(master)\n",
        "out_tr.to_pickle(\"X_train_transformed.pkl\")\n",
        "y_train.to_pickle(\"y_train.pkl\")\n",
        "\n",
        "\n",
        "\n",
        "##############################\n",
        "## Start reading\n",
        "#############################\n",
        "\n",
        "cat_cols = ['Cone', 'bannerpos', 'siteid', 'sitedomain', 'sitecategory',\n",
        "            'appid', 'appdomain', 'appcategory', 'devicemodel', 'devicetype',\n",
        "            'deviceconntype', 'Cfourteen', 'Cfifteen','Csixteen', 'Cseventeen',\n",
        "            'Ceighteen', 'Cnineteen', 'Ctwenty', 'Ctwentyone']  # 19 cols\n",
        "\n",
        "rem_cols = ['day', 'dayofweek', 'device_id',\n",
        "            'device_ip', 'hour_time', 'month']  # 6 cols\n",
        "\n",
        "os.chdir(master)\n",
        "X_train_trans = pd.read_pickle(\"X_train_transformed.pkl\")\n",
        "X_test_trans = pd.read_pickle(\"X_test_transformed.pkl\")\n",
        "X_train = pd.read_pickle(\"X_train.pkl\")\n",
        "X_test = pd.read_pickle(\"X_test.pkl\")\n",
        "y_train = pd.read_pickle(\"y_train.pkl\")\n",
        "y_test = pd.read_pickle(\"y_test.pkl\")\n",
        "\n",
        "X_test_trans.columns[19:]\n",
        "X_test_trans.columns[:19]\n",
        "\n",
        "\n",
        "##************************\n",
        "## Predictive analytics\n",
        "##************************\n",
        "\n",
        "# Modeling with computed data\n",
        "# Concat remaining columns:\n",
        "X_train = X_train.reset_index( drop = True )\n",
        "X_test = X_test.reset_index(drop = True)\n",
        "\n",
        "# Ignore original cat cols in transformed data\n",
        "l = list(X_train_trans.columns)\n",
        "l[:19]     # First 19 cols are cat_cols\n",
        "l = l[19:] # Forget them\n",
        "l[:5]      # Recheck\n",
        "\n",
        "# Concat with rem_cols:\n",
        "X_train_trans = pd.concat([X_train[rem_cols], X_train_trans[l] ],  axis = 1)\n",
        "X_test_trans = pd.concat([X_test[rem_cols], X_test_trans[l] ], axis = 1)\n",
        "\n",
        "# Check:\n",
        "X_train_trans.shape   # (2250567, 586)\n",
        "X_test_trans.shape   # (750189, 586)\n",
        "X_train_trans.head()\n",
        "\n",
        "# Prepare xgboost model:\n",
        "gc.collect()\n",
        "evals_result= {}\n",
        "model = 0\n",
        "model = xgb.XGBClassifier( n_estimators= 1000,\n",
        "                           verbosity = 3,\n",
        "                           eta = 0.06,\n",
        "                           max_depth = 15,\n",
        "                           subsample = 0.8,\n",
        "                           evals_result = evals_result,\n",
        "                           random_state = rng,\n",
        "                           )\n",
        "\n",
        "\n",
        "tr_X =   X_train_trans[fe_1[:-15]]\n",
        "test_X = X_test_trans[fe_1[:-15]]\n",
        "ytrain = y_train        # Just renaming\n",
        "ytest = y_test          # Just renaming\n",
        "\n",
        "\n",
        "model.fit(tr_X, ytrain.values,\n",
        "          early_stopping_rounds = 100,   # 10% of n_estimators\n",
        "          eval_set=[ (test_X, ytest.values)],\n",
        "          eval_metric = ['auc']    # binary classification problem\n",
        "          )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model.best_score   #  0.756914(30l);  0.76074(60l)\n",
        "pred = model.predict(test_X)\n",
        "(pred == ytest).sum()/ytest.size   # 0.835550774 (30l)\n",
        "\n",
        "# Get impt features:\n",
        "# fe_1: Ordered features with impt > 0\n",
        "# fe_0: Feature with zero importance\n",
        "fe_1, fe_0 = utils.xg_impt_features(model,X_train_trans.columns)\n",
        "len(fe_1)   # 408\n",
        "len(fe_0)   # 178\n",
        "\n",
        "print(classification_report(ytest, pred))\n",
        "\"\"\"\n",
        "                  precision  recall  f1-score   support\n",
        "\n",
        "           0       0.84      0.99      0.91    622933\n",
        "           1       0.60      0.09      0.16    127256\n",
        "\n",
        "    accuracy                           0.84    750189\n",
        "   macro avg       0.72      0.54      0.54    750189\n",
        "weighted avg       0.80      0.84      0.78    750189\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "# Modeling with original untransformed data\n",
        "#  (except hour etc)\n",
        "#  With all the features:\n",
        "evals_result= {}\n",
        "model_or =0\n",
        "model_or = xgb.XGBClassifier( n_estimators= 1000,\n",
        "                           verbosity = 3,\n",
        "                           eta = 0.06,      # 0.06\n",
        "                           max_depth = 15,\n",
        "                           subsample = 0.8,           # 0.8\n",
        "                           evals_result = evals_result,\n",
        "                           random_state = rng,\n",
        "                           #enable_categorical = True\n",
        "                           )\n",
        "\n",
        "\n",
        "tr_X =   X_train\n",
        "test_X = X_test\n",
        "ytrain = y_train\n",
        "ytest =  y_test\n",
        "\n",
        "\n",
        "model_or.fit(tr_X, ytrain.values,\n",
        "          early_stopping_rounds = 100,   # 10% of n_estimators\n",
        "          eval_set=[ (test_X, ytest.values)],\n",
        "          eval_metric = ['auc']    # binary classification problem\n",
        "          )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model_or.best_score   #  0.7540460416\n",
        "pred = model_or.predict(test_X)\n",
        "(pred == ytest).sum()/ytest.size    #  0.835640; 0.8352290\n",
        "\n",
        "\n",
        "\n",
        "print(classification_report(ytest, pred))\n",
        "\"\"\"\n",
        "\n",
        "# -- End pasted text --\n",
        "                 precision    recall  f1-score   support\n",
        "\n",
        "           0       0.84      0.99      0.91    622933\n",
        "           1       0.59      0.09      0.16    127256\n",
        "\n",
        "    accuracy                           0.84    750189\n",
        "   macro avg       0.72      0.54      0.53    750189\n",
        "weighted avg       0.80      0.84      0.78    750189\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "#####################################################\n",
        "# SMOTE\n",
        "#####################################################\n",
        "\n",
        "from imblearn.over_sampling import SMOTE,ADASYN\n",
        "from imblearn.combine import SMOTEENN\n",
        "sm = SMOTE(random_state=rng)\n",
        "sm = ADASYN(random_state=rng)\n",
        "sm = SMOTEENN(random_state=rng)\n",
        "X_res, y_res = sm.fit_resample(X_train_trans, y_train)\n",
        "X_res.shape  #  (3768921, 586)\n",
        "y_res.shape  # ( (3768921, )\n",
        "\n",
        "# Save SMOTE data:\n",
        "os.chdir(master)\n",
        "X_res.to_pickle(\"X_res_transformed.pkl\")\n",
        "y_res.to_pickle(\"y_res.pkl\")\n",
        "\n",
        "# Read SMOTE data\n",
        "os.chdir(master)\n",
        "X_res = pd.read_pickle(\"X_res_transformed.pkl\")\n",
        "y_res = pd.read_pickle(\"y_res.pkl\")\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "# Modeling with smote data\n",
        "evals_result= {}\n",
        "model_sm =0\n",
        "model_sm = xgb.XGBClassifier( n_estimators= 300,\n",
        "                           verbosity = 3,\n",
        "                           eta = 0.06,      # 0.06\n",
        "                           max_depth = 15,\n",
        "                           subsample = 0.8,           # 0.8\n",
        "                           evals_result = evals_result,\n",
        "                           random_state = rng,\n",
        "                           #enable_categorical = True\n",
        "                           )\n",
        "\n",
        "\n",
        "tr_X =   X_res\n",
        "test_X = X_test_trans\n",
        "ytrain = y_res\n",
        "ytest =  y_test\n",
        "\n",
        "\n",
        "model_sm.fit(tr_X, ytrain.values,                   # Xtr, ytr\n",
        "          early_stopping_rounds = 100,   # 10% of n_estimators\n",
        "          eval_set=[ (test_X, ytest.values)],\n",
        "          eval_metric = ['auc']    # binary classification problem\n",
        "          )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model_sm.best_score   #  0.75517761 (30l)\n",
        "model_sm.best_iteration # 232\n",
        "pred = model_sm.predict(test_X)\n",
        "(pred == ytest).sum()/ytest.size    #  0.835498\n",
        "\n",
        "print(classification_report(ytest, pred))\n",
        "\n",
        "\"\"\"\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "           0       0.84      0.99      0.91    622933\n",
        "           1       0.59      0.10      0.17    127256\n",
        "\n",
        "    accuracy                           0.84    750189\n",
        "   macro avg       0.72      0.54      0.54    750189\n",
        "weighted avg       0.80      0.84      0.78    750189\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "score = model_sm.predict_proba(test_X)\n",
        "roc_auc_score(ytest, score[:,1])    #  0.7552722506975745\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "######################3\n",
        "\n",
        "\n",
        "seed = 678\n",
        "# Transformed data\n",
        "X_train, X_test, ytrain, ytest = train_test_split(\n",
        "                                                  train_trans[l],\n",
        "                                                  y_train,\n",
        "                                                  test_size = 0.25,\n",
        "                                                  random_state= seed)\n",
        "\n",
        "# original data\n",
        "X_train, X_test, ytrain, ytest = train_test_split(\n",
        "                                                  train,\n",
        "                                                  y_train,\n",
        "                                                  test_size = 0.25,\n",
        "                                                  random_state= seed)\n",
        "\n",
        "\n",
        "# PCA data\n",
        "Xtrain, Xtest, ytr, yte = train_test_split(da, y_train, test_size = 0.25 )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "del ss\n",
        "ss = StandardScaler()\n",
        "pca = PCA(n_components = 2, whiten = True, random_state=rng)\n",
        "da = pca.fit_transform(ss.fit_transform(train_trans))\n",
        "da.shape  # (32769, 2)\n",
        "db = pca.transform(ss.transform(test_trans_imputed))\n",
        "db.shape\n",
        "\n",
        "colnames = [\"c\" + str(i) for i in range(da.shape[1])]\n",
        "colnames\n",
        "da = pd.DataFrame(da, columns = colnames)\n",
        "db = pd.DataFrame(db, columns = colnames)\n",
        "\n",
        "n_train_trans = pd.concat([train_trans, da], axis = 1)\n",
        "n_train_trans.shape\n",
        "n_test_trans_imputed = pd.concat([test_trans_imputed, db], axis = 1)\n",
        "\n",
        "\n",
        "evals_result= {}\n",
        "model =0\n",
        "model = xgb.XGBClassifier( n_estimators= 1000,\n",
        "                           verbosity = 3,\n",
        "                           eta = 0.06,      # 0.06\n",
        "                           max_depth = 13,\n",
        "                           subsample = 0.8,           # 0.8\n",
        "                           evals_result = evals_result,\n",
        "                           random_state = rng,\n",
        "                           #enable_categorical = True\n",
        "                           )\n",
        "\n",
        "\n",
        "tr_X =  n_train_trans  #[fe_1[:-15]] # X_train # Xtrain\n",
        "test_X = n_test_trans_imputed   #[fe_1[:-15]] # X_test # Xtest\n",
        "ytrain = y_train\n",
        "ytest = y_test\n",
        "\n",
        "\n",
        "model.fit(tr_X, ytrain.values,                   # Xtr, ytr\n",
        "          early_stopping_rounds = 100,   # 10% of n_estimators\n",
        "          eval_set=[ (test_X, ytest.values)],\n",
        "          eval_metric = ['auc']    # binary classification problem\n",
        "          )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "gc.collect()\n",
        "from imblearn.over_sampling import SMOTE\n",
        "sm = SMOTE(random_state= rng)\n",
        "X_res, y_res = sm.fit_resample(train_trans, y_train)\n",
        "X_res.shape  # (746856, 131)\n",
        "\n",
        "\n",
        "\n",
        "evals_result= {}\n",
        "model1 =0\n",
        "model1 = xgb.XGBClassifier( n_estimators= 1000,\n",
        "                           verbosity = 3,\n",
        "                           eta = 0.06,      # 0.06\n",
        "                           max_depth = 11,\n",
        "                           subsample = 0.8,           # 0.8\n",
        "                           evals_result = evals_result,\n",
        "                           random_state = seed,\n",
        "                           #enable_categorical = True\n",
        "                           )\n",
        "\n",
        "\n",
        "tr_X =  X_res # Xtrain\n",
        "test_X = test_trans # Xtest\n",
        "ytrain = y_res\n",
        "ytest = y_test\n",
        "\n",
        "\n",
        "model1.fit(tr_X, ytrain.values,                   # Xtr, ytr\n",
        "          early_stopping_rounds = 100,   # 10% of n_estimators\n",
        "          eval_set=[ (test_X, ytest.values)],\n",
        "          eval_metric = ['auc', 'logloss']    # binary classification problem\n",
        "          )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model1.best_score   # 0.74345\n",
        "pred = model1.predict(test_X)\n",
        "(pred == ytest).sum()/ytest.size    # 0.83244\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "########### Resampling\n",
        "\n",
        "from sklearn.utils import resample\n",
        "\n",
        "train_trans['click'] = y_train\n",
        "\n",
        "train_trans.shape\n",
        "\n",
        "train_trans.columns\n",
        "\n",
        "# Separate majority and minority classes\n",
        "df_majority = train_trans[train_trans.click==0]\n",
        "df_majority.shape\n",
        "df_minority = train_trans[train_trans.click==1]\n",
        "df_minority.shape\n",
        "\n",
        "df_minority_upsampled = resample(df_minority,\n",
        "                                 replace=True,     # sample with replacement\n",
        "                                 n_samples=279050,    # to match majority class\n",
        "                                 random_state=123) # reproducible results\n",
        "\n",
        "\n",
        "df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
        "\n",
        "\n",
        "df_upsampled.click.value_counts()\n",
        "\n",
        "# Save it\n",
        "os,chdir(master)\n",
        "df_upsampled.to_pickle(\"df_upsampled.pkl\")\n",
        "os.chdir(master)\n",
        "df_upsampled = pd.read_pickle(\"df_upsampled.pkl\")\n",
        "\n",
        "y_tr = df_upsampled.pop('click')\n",
        "\n",
        "seed = 789\n",
        "evals_result= {}\n",
        "model =0\n",
        "model = xgb.XGBClassifier( n_estimators= 1000,\n",
        "                           verbosity = 3,\n",
        "                           eta = 0.06,      # 0.06\n",
        "                           max_depth = 11,\n",
        "                           subsample = 0.8,           # 0.8\n",
        "                           evals_result = evals_result,\n",
        "                           random_state = seed,\n",
        "                           #enable_categorical = True\n",
        "                           )\n",
        "\n",
        "\n",
        "tr_X = df_upsampled\n",
        "test_X = test_trans # Xtest\n",
        "ytrain = y_tr\n",
        "ytest = y_test\n",
        "\n",
        "\n",
        "model.fit(tr_X, ytrain.values,                   # Xtr, ytr\n",
        "          early_stopping_rounds = 100,   # 10% of n_estimators\n",
        "          eval_set=[ (test_X, ytest.values)],\n",
        "          eval_metric = ['auc']    # binary classification problem\n",
        "          )\n",
        "\n",
        "\n",
        "\n",
        "model.best_score   # 0.74345\n",
        "pred = model.predict(test_X)\n",
        "(pred == ytest).sum()/ytest.size    # 0.83244\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### NOT DONE BELOW\n",
        "##############################\n",
        "## tsne\n",
        "##############################\n",
        "# Why blobs do not appear together in tsne?\n",
        "# See StackOverflow:\n",
        "#    https://stats.stackexchange.com/a/453106/78454\n",
        "\n",
        "\n",
        "from sklearn.manifold import  TSNE\n",
        "\n",
        "\n",
        "## 2D\n",
        "tsne = TSNE()\n",
        "dx = tsne.fit_transform(orig_train)\n",
        "y_train.values.shape\n",
        "\n",
        "\n",
        "sns.scatterplot(x= dx[:,0], y = dx[:,1], hue = y_train.values)\n",
        "\n",
        "tsne = TSNE()\n",
        "org_trans_train.columns[20:]\n",
        "da = tsne.fit_transform(org_trans_train[org_trans_train.columns[20:]])\n",
        "da.shape\n",
        "sns.scatterplot(x= da[:,0], y = da[:,1], hue = y_train.values)\n",
        "sns.scatterplot(x= dx[:,0], y = dx[:,1], hue = y_train.values)\n",
        "\n",
        "\n",
        "## 3D\n",
        "tsne = TSNE(n_components = 3, early_exaggeration = 40)\n",
        "dx3 = tsne.fit_transform(orig_train)\n",
        "dx3.shape\n",
        "\n",
        "\n",
        "tsne = TSNE(n_components=3)\n",
        "org_trans_train.columns[20:]\n",
        "da3 = tsne.fit_transform(org_trans_train[org_trans_train.columns[20:]])\n",
        "da3.shape\n",
        "\n",
        "colnames = [\"c\" + str(i) for i in range(dx3.shape[1])]\n",
        "colnames\n",
        "dx3 = pd.DataFrame(dx3, columns = colnames)\n",
        "da3 = pd.DataFrame(da3, columns = colnames)\n",
        "\n",
        "dx3['target'] = y_train\n",
        "da3['target'] = y_train\n",
        "dx3.head()\n",
        "da3.head()\n",
        "\n",
        "os.chdir(master)\n",
        "dx3.to_csv(\"dx3.csv\", index = False)\n",
        "da3.to_csv(\"da3.csv\", index = False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train, X_test, ytrain, ytest = train_test_split(dx3.iloc[:,:3], y_train, test_size = 0.25 )\n",
        "Xtrain, Xtest, ytr, yte = train_test_split(da3.iloc[:,:3], y_train, test_size = 0.25 )\n",
        "\n",
        "evals_result= {}\n",
        "model_tsne = xgb.XGBClassifier( n_estimators= 1000,\n",
        "                           verbosity = 3,\n",
        "                           eta = 0.06,      # 0.06\n",
        "                           max_depth = 6,\n",
        "                           subsample = 0.8,           # 0.8\n",
        "                           evals_result = evals_result,\n",
        "                           random_state = 70\n",
        "                           )\n",
        "\n",
        "\n",
        "tr_X =  X_train\n",
        "test_X = X_test\n",
        "\n",
        "\n",
        "\n",
        "model_tsne.fit(tr_X, ytrain.values,                   # Xtr, ytr\n",
        "          early_stopping_rounds = 100,   # 10% of n_estimators\n",
        "          eval_set=[ (test_X, ytest.values)],\n",
        "          eval_metric = ['auc']\n",
        "          )\n",
        "\n",
        "\n",
        "\n",
        "# auc: 0.81646\n",
        "model_tsne.best_score   # 1.096898\n",
        "pred = model_tsne.predict(test_X)\n",
        "(pred == yte).sum()/yte.size    # 0.75\n",
        "\n",
        "\n",
        "\n",
        "##############################\n",
        "## umap\n",
        "##############################\n",
        "\n",
        "## 2D\n",
        "\n",
        "reducer = umap.UMAP()\n",
        "ss = StandardScaler()\n",
        "dx = reducer.fit_transform(ss.fit_transform(orig_train))\n",
        "\n",
        "sns.scatterplot(x= dx[:,0], y = dx[:,1], hue = y_train.values)\n",
        "\n",
        "reducer = umap.UMAP()\n",
        "ss = StandardScaler()\n",
        "org_trans_train.columns[20:]\n",
        "da = reducer.fit_transform(ss.fit_transform(org_trans_train[org_trans_train.columns[20:]]))\n",
        "da.shape\n",
        "sns.scatterplot(x= da[:,0], y = da[:,1], hue = y_train.values)\n",
        "sns.scatterplot(x= dx[:,0], y = dx[:,1], hue = y_train.values)\n",
        "\n",
        "\n",
        "colnames = [\"c\" + str(i) for i in range(dx.shape[1])]\n",
        "colnames\n",
        "dx = pd.DataFrame(dx, columns = colnames)\n",
        "da = pd.DataFrame(da, columns = colnames)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train, X_test, ytrain, ytest = train_test_split(dx, y_train, test_size = 0.25 )\n",
        "Xtrain, Xtest, ytr, yte = train_test_split(da, y_train, test_size = 0.25 )\n",
        "\n",
        "evals_result= {}\n",
        "model_umap = xgb.XGBClassifier( n_estimators= 1000,\n",
        "                           verbosity = 3,\n",
        "                           eta = 0.06,      # 0.06\n",
        "                           max_depth = 6,\n",
        "                           subsample = 0.8,           # 0.8\n",
        "                           evals_result = evals_result,\n",
        "                           random_state = 70\n",
        "                           )\n",
        "\n",
        "\n",
        "tr_X =  Xtrain\n",
        "test_X = Xtest\n",
        "\n",
        "\n",
        "\n",
        "model_umap.fit(tr_X, ytr.values,\n",
        "          early_stopping_rounds = 100,   # 10% of n_estimators\n",
        "          eval_set=[ (test_X, yte.values)],\n",
        "          eval_metric = ['auc']\n",
        "          )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model_umap.best_score\n",
        "pred = model_pca.predict(test_X)\n",
        "(pred == yte).sum()/yte.size\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#########################################\n",
        "## Predictive analytics\n",
        "########################################\n",
        "# Call it only once\n",
        "# See https://scikit-learn.org/stable/common_pitfalls.html#general-recommendations\n",
        "\n",
        "\n",
        "model0 = 0\n",
        "gc.collect()\n",
        "del model0\n",
        "evals_result= {}\n",
        "model0 = xgb.XGBClassifier( n_estimators= 1000,\n",
        "                           verbosity = 3,\n",
        "                           eta = 0.06,      # 0.06\n",
        "                           max_depth = 6,\n",
        "                           subsample = 0.8,           # 0.8\n",
        "                           evals_result = evals_result,\n",
        "                           random_state = seed\n",
        "                           )\n",
        "\n",
        "\n",
        "tr_X =  org_trans_train\n",
        "test_X =  org_trans_test\n",
        "\n",
        "\n",
        "\n",
        "model0.fit(tr_X, y_train.values,                   # Xtr, ytr\n",
        "          early_stopping_rounds = 100,   # 10% of n_estimators\n",
        "          eval_set=[ (test_X, y_test.values)],\n",
        "          eval_metric = ['auc']\n",
        "          )\n",
        "\n",
        "\n",
        "\n",
        "# auc: 0.81646\n",
        "model0.best_score   # 0.81761; 820858; 0.816837; 0.892089; 0.876738; 0.884359; 0.885373\n",
        "                    # 0.84595; 0.851114\n",
        "pred = model0.predict(test_X)\n",
        "(pred == y_test).sum()/y_test.size    # 0.7324 0.8022; 0.78395; 0.7954\n",
        "                                      # 0.7664;0.7716\n",
        "#plot_importance(model, importance_type = 'gain')\n",
        "\n",
        "\n",
        "\n",
        "fe_1, fe_0 = xg_impt_features(model0,org_trans_train.columns  )\n",
        "\n",
        "len(fe_1)   # 335  86  55 76   77  88\n",
        "len(fe_0)   # 743  11  11 14   16  16\n",
        "\n",
        "\n",
        "\n",
        "os.chdir(master)\n",
        "file = open('fe_1.txt','w')\n",
        "for  item in fe_1:\n",
        "\tfile.write(item+\"\\n\")\n",
        "file.close()\n",
        "\n",
        "# Read fe_1\n",
        "os.chdir(master)\n",
        "with open(\"fe_1.txt\", 'r') as f:\n",
        "    fe_1 = [line.rstrip('\\n') for line in f]\n",
        "\n",
        "len(fe_1)  # 77  88\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##---------------\n",
        "# With reduced best features\n",
        "model1 = 0\n",
        "gc.collect()\n",
        "del model1\n",
        "evals_result= {}\n",
        "model1 = xgb.XGBClassifier( n_estimators= 1000,\n",
        "                           verbosity = 3,\n",
        "                           eta = 0.06,      # 0.06\n",
        "                           max_depth = 6,\n",
        "                           subsample = 0.8,           # 0.8\n",
        "                           evals_result = evals_result,\n",
        "                           random_state = seed\n",
        "                           )\n",
        "\n",
        "\n",
        "tr_X =  org_trans_train[fe_1[:15]]     # Try from 7 to 30\n",
        "test_X =  org_trans_test[fe_1[:15]]\n",
        "\n",
        "\n",
        "\n",
        "model1.fit(tr_X, y_train.values,                   # Xtr, ytr\n",
        "          early_stopping_rounds = 100,   # 10% of n_estimators\n",
        "          eval_set=[ (test_X, y_test.values)],\n",
        "          eval_metric = ['auc']\n",
        "          )\n",
        "\n",
        "\n",
        "# auc: 0.81646\n",
        "model1.best_score   # 0.7228\n",
        "\n",
        "pred = model1.predict(test_X)\n",
        "(pred == y_test).sum()/y_test.size    # 0.5244\n",
        "\n",
        "\n",
        "fe_1[:6]\n",
        "\n",
        "\n",
        "fe_1[:7]\n",
        "\n",
        "##--------------------\n",
        "# orig + binned\n",
        "##--------------------\n",
        "gc.collect()\n",
        "#del model\n",
        "evals_result= {}\n",
        "model2 = xgb.XGBClassifier( n_estimators= 1000,\n",
        "                           verbosity = 3,\n",
        "                           eta = 0.06,      # 0.06\n",
        "                           max_depth = 6,\n",
        "                           subsample = 0.8,           # 0.8\n",
        "                           evals_result = evals_result,\n",
        "                           random_state = seed\n",
        "                           )\n",
        "\n",
        "\n",
        "tr_X =  org_binned_train\n",
        "test_X =  org_binned_test\n",
        "\n",
        "\n",
        "\n",
        "model2.fit(tr_X, y_train.values,                   # Xtr, ytr\n",
        "          early_stopping_rounds = 50,   # 10% of n_estimators\n",
        "          eval_set=[ (test_X, y_test.values)],\n",
        "          eval_metric = ['auc']\n",
        "          )\n",
        "\n",
        "\n",
        "\n",
        "# auc: 0.81646\n",
        "model2.best_score   # 0.821435 ; 827361 ; 0.897\n",
        "pred = model2.predict(test_X)\n",
        "(pred == y_test).sum()/y_test.size    # 0.7324 ; 0.81\n",
        "\n",
        "fe_11, fe_00 = xg_impt_features(model2,org_binned_train.columns  )\n",
        "len(fe_11)\n",
        "fe_00\n",
        "\n",
        "##-------------------\n",
        "# orig + binned best features\n",
        "##-------------------\n",
        "\n",
        "\n",
        "gc.collect()\n",
        "#del model\n",
        "evals_result= {}\n",
        "model3 = xgb.XGBClassifier( n_estimators= 1000,\n",
        "                           verbosity = 3,\n",
        "                           eta = 0.06,      # 0.06\n",
        "                           max_depth = 6,\n",
        "                           subsample = 0.8,           # 0.8\n",
        "                           evals_result = evals_result,\n",
        "                           random_state = rng\n",
        "                           )\n",
        "\n",
        "\n",
        "tr_X =  org_binned_train[fe_11]\n",
        "test_X =  org_binned_test[fe_11]\n",
        "\n",
        "\n",
        "\n",
        "model3.fit(tr_X, y_train.values,                   # Xtr, ytr\n",
        "          early_stopping_rounds = 50,   # 10% of n_estimators\n",
        "          eval_set=[ (test_X, y_test.values)],\n",
        "          eval_metric = ['auc']\n",
        "          )\n",
        "\n",
        "\n",
        "\n",
        "# auc: 0.81646\n",
        "model3.best_score   # 826236; 826423\n",
        "pred = model3.predict(test_X)\n",
        "(pred == y_test).sum()/y_test.size    # 0.7324\n",
        "\n",
        "\n",
        "\n",
        "##--------------------\n",
        "##-------------------\n",
        "# orig  features\n",
        "##-------------------\n",
        "\n",
        "\n",
        "model4 = 0\n",
        "\n",
        "gc.collect()\n",
        "del model4\n",
        "evals_result= {}\n",
        "model4 = xgb.XGBClassifier( n_estimators= 1000,\n",
        "                           verbosity = 3,\n",
        "                           eta = 0.06,      # 0.06\n",
        "                           max_depth = 6,\n",
        "                           subsample = 0.8,           # 0.8\n",
        "                           evals_result = evals_result,\n",
        "                           random_state = seed\n",
        "                           )\n",
        "\n",
        "\n",
        "tr_X =  orig_train[fe_4_1[:5]]\n",
        "test_X =  orig_test[fe_4_1[:5]]\n",
        "\n",
        "\n",
        "\n",
        "model4.fit(tr_X, y_train.values,                   # Xtr, ytr\n",
        "          early_stopping_rounds = 100,   # 10% of n_estimators\n",
        "          eval_set=[ (test_X, y_test.values)],\n",
        "          eval_metric = ['auc']\n",
        "          )\n",
        "\n",
        "\n",
        "\n",
        "# auc: 0.81646\n",
        "model4.best_score   # 0.7335065739582236\n",
        "pred = model4.predict(test_X)\n",
        "(pred == y_test).sum()/y_test.size    # 0.544\n",
        "\n",
        "fe_4_1, fe_4_0 = xg_impt_features(model4,orig_train.columns  )\n",
        "\n",
        "fe_4_1[:5]\n",
        "\n",
        "##--------------------\n",
        "\n",
        "fe_4_1[:5]\n",
        "\n",
        "model4_1 = 0\n",
        "\n",
        "gc.collect()\n",
        "del model4_1\n",
        "evals_result= {}\n",
        "model4_1 = xgb.XGBClassifier( n_estimators= 1000,\n",
        "                           verbosity = 3,\n",
        "                           eta = 0.06,      # 0.06\n",
        "                           max_depth = 6,\n",
        "                           subsample = 0.8,           # 0.8\n",
        "                           evals_result = evals_result,\n",
        "                           random_state = 70\n",
        "                           )\n",
        "\n",
        "\n",
        "tr_X =  orig_train[fe_4_1[:5]]\n",
        "test_X =  orig_test[fe_4_1[:5]]\n",
        "\n",
        "\n",
        "\n",
        "model4_1.fit(tr_X, y_train.values,                   # Xtr, ytr\n",
        "          early_stopping_rounds = 100,   # 10% of n_estimators\n",
        "          eval_set=[ (test_X, y_test.values)],\n",
        "          eval_metric = ['auc']\n",
        "          )\n",
        "\n",
        "\n",
        "\n",
        "# auc: 0.81646\n",
        "model4_1.best_score   # 831523 ; 824436 ; 0.8288 ; 0.897301 ; 0.880147; (0.891444, 0.892768, 0.893049)\n",
        "                    # (0.858484,0.862771, 0.874083 )\n",
        "pred = model4_1.predict(test_X)\n",
        "(pred == y_test).sum()/y_test.size    # 0.7376 ; 0.81; 0.7881; 0.8014, 0.8044\n",
        "                                      # 0.7788; 0.7918\n",
        "\n",
        "###################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###################################\n",
        "####################################\n",
        "\n",
        "\n",
        "y = train_train.pop('target')\n",
        "train_train.head()\n",
        "ohe = OneHotEncoder(  sparse = False)\n",
        "ohe.fit(train_train)\n",
        "train_ohe = ohe.transform(train_train)\n",
        "train_ohe.shape  # (7500, 89)\n",
        "cl = [\"c\" + str(i) for i in range(train_ohe.shape[1]) ]\n",
        "train_ohe = pd.DataFrame(train_ohe,columns = cl)\n",
        "train_ohe.head()\n",
        "train_ohe.shape  # (7500,75)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "pca = PCA(n_components=3)\n",
        "train_pca= pca.fit_transform(train_ohe)\n",
        "train_ohe.head()\n",
        "cx = [\"c\" + str(i) for i in range(train_pca.shape[1]) ]\n",
        "train_pca = pd.DataFrame(train_pca,columns = cx)\n",
        "train_pca.head()\n",
        "\n",
        "\n",
        "\n",
        "os.chdir(dataPath)\n",
        "\n",
        "train_pca.to_csv(\"train_pca.csv\", index = False)\n",
        "y.to_csv(\"y_train_pca.csv\", index = False)\n",
        "y.head()\n",
        "\n",
        "\n",
        "##################Model with orig data #####################\n",
        "\n",
        "\n",
        "X = orig_train\n",
        "y = orig_train.pop('target')\n",
        "X.columns\n",
        "X.head()\n",
        "y\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split( X,y,\n",
        "                                                 test_size = 0.25,\n",
        "                                                 stratify = y,\n",
        "                                                 random_state = 384)\n",
        "\n",
        "gc.collect()\n",
        "#del model\n",
        "evals_result= {}\n",
        "model = xgb.XGBClassifier( n_estimators= 700,\n",
        "                           verbosity = 3,\n",
        "                           eta = 0.06,      # 0.06\n",
        "                           max_depth = 6,\n",
        "                           subsample = 0.8,           # 0.8\n",
        "                           evals_result = evals_result,\n",
        "                           random_state = 800\n",
        "                           )\n",
        "\n",
        "tr_X =  X_train\n",
        "test_X =  X_test\n",
        "\n",
        "\n",
        "model.fit(tr_X, y_train,                   # Xtr, ytr\n",
        "          early_stopping_rounds = 50,   # 10% of n_estimators\n",
        "          eval_set=[ (test_X, y_test)],\n",
        "          eval_metric = ['merror']\n",
        "          )\n",
        "\n",
        "\n",
        "\n",
        "pred = model.predict(test_X)\n",
        "(pred == y_test).sum()/y_test.size    # 94.93%   91.8%  94.73  98.2(class_Sep = 2.0)\n",
        "plot_importance(model, importance_type = 'gain')\n",
        "\n",
        "################## Model with discrete features #####################\n",
        "\n",
        "\n",
        "X = train_train\n",
        "y = train_train.pop('target')\n",
        "X.columns\n",
        "X.head()\n",
        "y\n",
        "\n",
        "for i,j in enumerate(X.columns):\n",
        "    X[j] = X[j].astype('int')\n",
        "\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split( X,y,\n",
        "                                                 test_size = 0.25,\n",
        "                                                 stratify = y,\n",
        "                                                 random_state = 384)\n",
        "\n",
        "gc.collect()\n",
        "del model\n",
        "evals_result= {}\n",
        "model = xgb.XGBClassifier( n_estimators= 700,\n",
        "                           verbosity = 3,\n",
        "                           eta = 0.06,      # 0.06\n",
        "                           max_depth = 6,\n",
        "                           subsample = 0.8,           # 0.8\n",
        "                           evals_result = evals_result,\n",
        "                           random_state = 800\n",
        "                           )\n",
        "\n",
        "tr_X =  X_train\n",
        "test_X =  X_test\n",
        "\n",
        "\n",
        "model.fit(tr_X, y_train,                   # Xtr, ytr\n",
        "          early_stopping_rounds = 50,   # 10% of n_estimators\n",
        "          eval_set=[ (test_X, y_test)],\n",
        "          eval_metric = ['merror']\n",
        "          )\n",
        "\n",
        "\n",
        "\n",
        "pred = model.predict(test_X)\n",
        "(pred == y_test).sum()/y_test.size    # 94.6% ; 95%  90.8%  94.86  98.86(class sep = 2.0)\n",
        "plot_importance(model, importance_type = 'gain')\n",
        "\n",
        "##############################################################\n",
        "\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "fig,ax= plt.subplots(1,1,figsize = (10,10))\n",
        "sns.scatterplot(data = tr_X, x = 'fe', y = 'fd', hue= y_train, ax = ax, alpha = 0.4)\n",
        "\n",
        "fig,ax= plt.subplots(1,1,figsize = (10,10))\n",
        "sns.scatterplot(data = orig_train, x = 'fe', y = 'fb', hue= y,ax=ax ,palette = \"Set2\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#################################################################\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(1)\n",
        "plt.clf()\n",
        "colors = [\"#dede00\", \"#377eb8\", \"#f781bf\"]\n",
        "markers = [\"x\", \"o\", \"^\"]\n",
        "\n",
        "# Three clusters can be seen\n",
        "fig = plt.figure(figsize = (8,8))\n",
        "_=sns.scatterplot(data = X, x = \"x1\", y = \"x2\", hue = y)\n",
        "\n",
        "fig = plt.figure(figsize = (8,8))\n",
        "_=sns.scatterplot(data = X, x = \"x2\", y = \"x3\", hue = y)\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize = (8,8)) ;\n",
        "_=sns.scatterplot(data = X, x = \"x1\", y = \"x3\", hue = y)"
      ]
    }
  ]
}