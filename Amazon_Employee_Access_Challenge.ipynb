{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNC9XpHgvmOycRHfpiPoYrj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harnalashok/CatEncodersFamily/blob/main/Amazon_Employee_Access_Challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "16th May, 2023\n",
        "\n",
        "\n",
        "Keep 'resource' aside while transforming features\n",
        "Still get comparable results to xgboost\n",
        "on full data with 'resource'\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "x6gdOLw4m15g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%reset -f\n",
        "\n",
        "# 1.0 Call libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "# 1.01\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "#import umap  # Takes long time to import\n",
        "import seaborn as sns\n",
        "import xgboost as xgb\n",
        "\n",
        "\n",
        "\n",
        "# 1.02 Misc\n",
        "import os,gc , time\n",
        "\n",
        "\n",
        "# 1.03 Home made modules\n",
        "\n",
        "os.chdir(\"C:\\\\Users\\\\Ashok\\\\OneDrive\\\\Documents\\\\talkingdata\\\\26042023_amazon\\\\\")\n",
        "import utils\n",
        "from utils import *\n",
        "import catfamilyenc\n",
        "\n",
        "# 1.04\n",
        "\n",
        "import importlib; importlib.reload(utils)\n",
        "import importlib; importlib.reload(catfamilyenc)\n",
        "\n"
      ],
      "metadata": {
        "id": "Ul9J_i2Im7Ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RzdkOebIoL4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import PyDrive and associated libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# List .txt files in the root.\n",
        "#\n",
        "# Search query reference:\n",
        "# https://developers.google.com/drive/v2/web/search-parameters\n",
        "listed = drive.ListFile({'q': \"title contains '.txt' and 'root' in parents\"}).GetList()\n",
        "for file in listed:\n",
        "  print('title {}, id {}'.format(file['title'], file['id']))"
      ],
      "metadata": {
        "id": "Tqcyqy7qoZLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.05\n",
        "dataPath =                 \"C:\\\\Users\\\\Ashok\\\\OneDrive\\\\Documents\\\\talkingdata\\\\26042023_amazon\\\\\"\n",
        "modelsPath =               \"C:\\\\Users\\\\Ashok\\\\OneDrive\\\\Documents\\\\talkingdata\\\\26042023_amazon\\\\allmodels\\\\models\\\\\"\n",
        "pathToStoreProgress =      \"C:\\\\Users\\\\Ashok\\\\OneDrive\\\\Documents\\\\talkingdata\\\\26042023_amazon\\\\allmodels\\\\progress\\\\\"\n",
        "master =  dataPath + \"master\\\\\"\n",
        "os.chdir(dataPath)"
      ],
      "metadata": {
        "id": "Llx5ABvPm_0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 2.0 Decide program-wide seed\n",
        "rng= np.random.RandomState(0)\n"
      ],
      "metadata": {
        "id": "K9QIwVB0nG5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read Data\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "test = pd.read_csv(\"test.csv\")   # Does not contain action columns"
      ],
      "metadata": {
        "id": "Yo_wF6XsnZju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.head()\n",
        "train.head()\n",
        "train.shape     # (32769, 10)\n",
        "train.columns"
      ],
      "metadata": {
        "id": "oGfu3-hjncvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y = train.pop(\"action\")\n",
        "test.pop(\"id\")\n",
        "\n",
        "# Map target to 0 and 1\n",
        "y.value_counts()\n",
        "y.value_counts(normalize = True)  # 95%:6%\n",
        "\n",
        "\n",
        "# Check nulls. None.\n",
        "train.isnull().sum()\n",
        "test.isnull().sum()\n",
        "\n"
      ],
      "metadata": {
        "id": "rBDXarTGnL74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WFYn8Czmokl"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Rename columns with spaces:\n",
        "#data = data.rename(columns = { \"concave points_mean\" : \"concave_points_mean\",\n",
        "#                     \"concave points_se\" : \"concave_points_se \",\n",
        "#                     \"concave points_worst\" : \"concave_points_worst\"\n",
        "#                    }\n",
        "#                   )\n",
        "\n",
        "print(train.columns)\n",
        "len(train.columns)    # 9\n",
        "\n",
        "\n",
        "## Developing models\n",
        "# Which are our cat columns\n",
        "# We will consider few columns\n",
        "# Ref: https://www.kaggle.com/code/kanncaa1/feature-selection-and-data-visualization\n",
        "\n",
        "cat_cols = ['mgrid', 'rolerollupOne', 'rolerolluptwo', 'roledeptname',\n",
        "           'roletitle', 'rolefamilydesc', 'rolefamily', 'rolecode']\n",
        "cat_cols\n",
        "len(cat_cols)  # 8. 'resource' is not included\n",
        "train['resource'].value_counts()\n",
        "train['resource'].nunique()   # 7518\n",
        "\n",
        "\n",
        "X_train, X_test, y_train,y_test = train_test_split(train, y, test_size=0.25, random_state=rng)\n",
        "\n",
        "X_train.shape  #  (24576, 9)\n",
        "X_test.shape   #  (8193, 9)\n",
        "y_train.shape  # (24576,)\n",
        "y_test.shape   # (8193,)\n",
        "\n",
        "# Save these for future\n",
        "os.chdir(master)\n",
        "X_train.to_pickle(\"X_train.pkl\")\n",
        "X_test.to_pickle(\"X_test.pkl\")\n",
        "y_train.to_pickle(\"y_train.pkl\")\n",
        "y_test.to_pickle(\"y_test.pkl\")\n",
        "# Read the data back:\n",
        "os.chdir(master)\n",
        "X_train = pd.read_pickle(\"X_train.pkl\")\n",
        "X_test = pd.read_pickle(\"X_test.pkl\")\n",
        "y_train= pd.read_pickle(\"y_train.pkl\")\n",
        "y_test = pd.read_pickle(\"y_test.pkl\")\n",
        "\n",
        "X_train.shape  #  (24576, 9)\n",
        "X_test.shape   #  (8193, 9)\n",
        "y_train.shape  # (24576,)\n",
        "y_test.shape   # (8193,)\n",
        "\n",
        "\n",
        "# Keep interacting columns, same\n",
        "# as cat columns:\n",
        "interactingCatCols = []\n",
        "\n",
        "# Instantiate CustomTransformer class:\n",
        "ct = catfamilyenc.CatFamilyEncoder( cMeasures=[1,1,1,0,None,1,1],\n",
        "                       noOfColsToConcat = 2,\n",
        "                       n_iter =1,\n",
        "                       k = 40,     # Does it matter here?\n",
        "                       modelsPath = modelsPath,\n",
        "                       pathToStoreProgress = pathToStoreProgress,\n",
        "                       saveGraph = True\n",
        "                       )\n",
        "\n",
        "# Fit it:\n",
        "ct.fit(X_train,  cat_cols, interactingCatCols)\n",
        "\n",
        "\n",
        "utils.savePythonObject(ct, \"transformer.pkl\", modelsPath)\n",
        "del ct\n",
        "ct = utils.restorePythonObject(\"transformer.pkl\", modelsPath)\n",
        "ct\n",
        "\n",
        "\n",
        "ct.modelsPath\n",
        "ct.plotNetworkGraph(\"mgrid_projected_roledeptname.gml\",\n",
        "                    figsize = (10,10),\n",
        "                    connected_nodes = True\n",
        "                    )\n",
        "\n",
        "\n",
        "\n",
        "# Transform test_binned data with only cat_cols\n",
        "out_te = ct.transform(X_test)\n",
        "out_te.shape      #   (8193, 219)\n",
        "out_te.columns\n",
        "# Remove low variance columns\n",
        "#out_te = utils.removeLowVarCols( out_te , pca = False)\n",
        "#out_te.shape   #    (8193, 71)\n",
        "y_test.shape   #\n",
        "out_te.columns    # (8193,)\n",
        "\n",
        "os.chdir(master)\n",
        "out_te.to_pickle(\"X_test_transformed.pkl\")\n",
        "y_test.to_pickle(\"y_test.pkl\")\n",
        "\n",
        "\n",
        "\n",
        "# Check list of original columns\n",
        "gc.collect()\n",
        "out_tr = ct.transform(X_train)\n",
        "\n",
        "\n",
        "os.chdir(master)\n",
        "out_tr.to_pickle(\"X_train_transformed.pkl\")\n",
        "y_train.to_pickle(\"y_train.pkl\")\n",
        "out_te.shape   #   (8193, 219)\n",
        "out_tr.shape   #   (24576, 219)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##############################\n",
        "## Start reading\n",
        "#############################\n",
        "\n",
        "os.chdir(master)\n",
        "train_trans = pd.read_pickle(\"X_train_transformed.pkl\")\n",
        "test_trans = pd.read_pickle(\"X_test_transformed.pkl\")\n",
        "X_train = pd.read_pickle(\"X_train.pkl\")\n",
        "X_test = pd.read_pickle(\"X_test.pkl\")\n",
        "y_train = pd.read_pickle(\"y_train.pkl\")\n",
        "y_test=pd.read_pickle(\"y_test.pkl\")\n",
        "\n",
        "train_trans.shape   #   (24576, 219)\n",
        "test_trans.shape    #  (8193, 219)\n",
        "train_trans.columns\n",
        "\n",
        "\n",
        "train_trans.columns[:9]\n",
        "l=train_trans.columns[9:]\n",
        "l\n",
        "\n",
        "\n",
        "#############################3\n",
        "\n",
        "\n",
        "\n",
        "model= 0\n",
        "evals_result= {}\n",
        "del model\n",
        "model = xgb.XGBClassifier( n_estimators= 1000,\n",
        "                           verbosity = 3,\n",
        "                           eta = 0.04,      # 0.06\n",
        "                           max_depth = 13,\n",
        "                           subsample = 0.9,           # 0.8\n",
        "                           evals_result = evals_result,\n",
        "                           random_state = rng,\n",
        "                           reg_lambda = 1.5,\n",
        "\n",
        "\n",
        "                           )\n",
        "\n",
        "## NOTE THIS IS WITHOT resource column\n",
        "tr_X =  train_trans[l] # Xtrain\n",
        "test_X = test_trans[l] # Xtest\n",
        "ytrain = y_train\n",
        "ytest = y_test\n",
        "\n",
        "\n",
        "model.fit(tr_X, ytrain.values,                   # Xtr, ytr\n",
        "          early_stopping_rounds = 100,   # 10% of n_estimators\n",
        "          eval_set=[ (test_X, ytest.values)],\n",
        "          eval_metric = ['auc']    # binary classification problem\n",
        "          )\n",
        "\n",
        "\n",
        "\n",
        "# auc: 0.81646\n",
        "model.best_score   # 0.8622657\n",
        "model.best_iteration # 155\n",
        "pred = model.predict(test_X)\n",
        "(pred == ytest).sum()/ytest.size    # 0.948858\n",
        "\n",
        "print(classification_report(ytest,pred))\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.59      0.28      0.38       458\n",
        "           1       0.96      0.99      0.97      7735\n",
        "\n",
        "    accuracy                           0.95      8193\n",
        "   macro avg       0.77      0.63      0.68      8193\n",
        "weighted avg       0.94      0.95      0.94      8193\n",
        "\n",
        "            precision    recall  f1-score   support\n",
        "\n",
        "           0       0.58      0.36      0.45       458\n",
        "           1       0.96      0.98      0.97      7735\n",
        "\n",
        "    accuracy                           0.95      8193\n",
        "   macro avg       0.77      0.67      0.71      8193\n",
        "weighted avg       0.94      0.95      0.94      8193\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "#################3\n",
        "\n",
        "model_or= 0\n",
        "evals_result= {}\n",
        "del model_or\n",
        "model_or = xgb.XGBClassifier( n_estimators= 1000,\n",
        "                           verbosity = 3,\n",
        "                           eta = 0.06,      # 0.06\n",
        "                           max_depth = 15,\n",
        "                           subsample = 0.8,           # 0.8\n",
        "                           evals_result = evals_result,\n",
        "                           random_state = rng\n",
        "                           )\n",
        "\n",
        "\n",
        "tr_X = X_train # Xtrain\n",
        "test_X = X_test # Xtest\n",
        "ytrain = y_train\n",
        "ytest = y_test\n",
        "\n",
        "\n",
        "model_or.fit(tr_X, ytrain.values,                   # Xtr, ytr\n",
        "          early_stopping_rounds = 100,   # 10% of n_estimators\n",
        "          eval_set=[ (test_X, ytest.values)],\n",
        "          eval_metric = ['auc']    # binary classification problem\n",
        "          )\n",
        "\n",
        "\n",
        "\n",
        "# auc: 0.81646\n",
        "model_or.best_score   # 0.8593978\n",
        "pred = model_or.predict(test_X)\n",
        "(pred == ytest).sum()/ytest.size    # 0.9515440\n",
        "\n",
        "print(classification_report(ytest.values,pred))\n",
        "\n",
        "\"\"\"\n",
        "               precision    recall  f1-score   support\n",
        "\n",
        "           0       0.64      0.27      0.38       458\n",
        "           1       0.96      0.99      0.97      7735\n",
        "\n",
        "    accuracy                           0.95      8193\n",
        "   macro avg       0.80      0.63      0.68      8193\n",
        "weighted avg       0.94      0.95      0.94      8193\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##############################\n",
        "## PCA\n",
        "##############################\n",
        "from sklearn.decomposition import PCA\n",
        "#from sklearn.decomposition import KernelPCA\n",
        "\n",
        "## 2D\n",
        "# kernelpca requires huge RAM. Hangs\n",
        "del pca\n",
        "pca = PCA(n_components= 2, whiten= True, random_state = rng)\n",
        "\n",
        "\n",
        "train_trans.columns[:9]\n",
        "l=train_trans.columns[9:]\n",
        "l\n",
        "\n",
        "# Check null status and fill it up with median\n",
        "da = train_trans[l]\n",
        "da.isnull().sum().sum()\n",
        "da.isnull().sum()[da.isnull().sum() > 0]\n",
        "nullcols = list(da.isnull().sum()[da.isnull().sum() > 0].index)\n",
        "nullcols\n",
        "# Fill up nulls using median\n",
        "for i in nullcols:\n",
        "    da[i]= da[i].fillna(da[i].median())\n",
        "\n",
        "# Check again\n",
        "da.isnull().sum().sum()\n",
        "da.columns\n",
        "da.shape   #  (513, 192)\n",
        "\n",
        "del ss\n",
        "ss = StandardScaler()\n",
        "da = pca.fit_transform(ss.fit_transform(da))\n",
        "da.shape  #  (24576, 19)\n",
        "\n",
        "colnames = [\"pc\" + str(i) for i in range(da.shape[1])]\n",
        "colnames\n",
        "da = pd.DataFrame(da, columns = colnames)\n",
        "sns.scatterplot(x= da['pc0'], y = da['pc1'], hue = y_train.values)\n",
        "\n",
        "## How good is PCA\n",
        "\n",
        "Xtrain, Xtest, ytr,yte = train_test_split(da, y_train, test_size = 0.20, stratify=y_train)\n",
        "\n",
        "model_pca= 0\n",
        "evals_result= {}\n",
        "del model_pca\n",
        "model_pca = xgb.XGBClassifier( n_estimators= 1000,\n",
        "                           verbosity = 3,\n",
        "                           eta = 0.06,      # 0.06\n",
        "                           max_depth = 15,\n",
        "                           subsample = 0.8,           # 0.8\n",
        "                           evals_result = evals_result,\n",
        "                           random_state = rng\n",
        "                           )\n",
        "\n",
        "\n",
        "tr_X = Xtrain # Xtrain\n",
        "test_X = Xtest # Xtest\n",
        "ytrain = ytr\n",
        "ytest = yte\n",
        "\n",
        "\n",
        "model_pca.fit(tr_X, ytrain.values,                   # Xtr, ytr\n",
        "          early_stopping_rounds = 100,   # 10% of n_estimators\n",
        "          eval_set=[ (test_X, ytest.values)],\n",
        "          eval_metric = ['auc']    # binary classification problem\n",
        "          )\n",
        "\n",
        "\n",
        "\n",
        "# auc: 0.81646\n",
        "model_pca.best_score   # 0.80\n",
        "pred = model_pca.predict(test_X)\n",
        "(pred == yte).sum()/yte.size    # 0.9515440\n",
        "\n",
        "print(classification_report(ytest.values,pred))\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "                 precision    recall  f1-score   support\n",
        "\n",
        "           0       0.57      0.16      0.25       288\n",
        "           1       0.95      0.99      0.97      4628\n",
        "\n",
        "    accuracy                           0.94      4916\n",
        "   macro avg       0.76      0.57      0.61      4916\n",
        "weighted avg       0.93      0.94      0.93      4916\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "##############################\n",
        "## tsne\n",
        "##############################\n",
        "# Why blobs do not appear together in tsne?\n",
        "# See StackOverflow:\n",
        "#    https://stats.stackexchange.com/a/453106/78454\n",
        "\n",
        "\n",
        "from sklearn.manifold import  TSNE\n",
        "\n",
        "# Not possible to tsne original data\n",
        "#  being categorical\n",
        "\n",
        "train_trans[l].head()\n",
        "tsne = TSNE(perplexity = 30)  # 30 gives best AUC\n",
        "                              # Tried 20 and 50\n",
        "ss = StandardScaler()\n",
        "da = tsne.fit_transform(ss.fit_transform(train_trans[l]))\n",
        "da.shape\n",
        "da\n",
        "colnames = [\"tsne\" + str(i) for i in range(da.shape[1])]\n",
        "colnames\n",
        "da = pd.DataFrame(da, columns = colnames)\n",
        "da.head()\n",
        "\n",
        "plt.figure(100)\n",
        "sns.scatterplot(x= da['tsne0'], y = da['tsne1'], hue = y_train.values)\n",
        "\n",
        "\n",
        "Xtrain, Xtest, ytr, yte = train_test_split(da, y_train, test_size = 0.20,stratify= y_train )\n",
        "\n",
        "evals_result= {}\n",
        "model_tsne = xgb.XGBClassifier( n_estimators= 1000,\n",
        "                           verbosity = 3,\n",
        "                           eta = 0.06,      # 0.06\n",
        "                           max_depth = 6,\n",
        "                           subsample = 0.8,           # 0.8\n",
        "                           evals_result = evals_result,\n",
        "                           random_state = 70\n",
        "                           )\n",
        "\n",
        "\n",
        "tr_X =  Xtrain\n",
        "test_X = Xtest\n",
        "\n",
        "\n",
        "\n",
        "model_tsne.fit(tr_X, ytr,                   # Xtr, ytr\n",
        "          early_stopping_rounds = 100,   # 10% of n_estimators\n",
        "          eval_set=[ (test_X, yte)],\n",
        "          eval_metric = ['auc']\n",
        "          )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model_tsne.best_score   # 0.8215140479448766\n",
        "pred = model_tsne.predict(test_X)\n",
        "(pred == yte).sum()/yte.size    # 0.75\n",
        "\n",
        "print(classification_report(yte,pred))\n",
        "\n",
        "\"\"\"\n",
        "auc = 0.82151\n",
        "\n",
        "                precision    recall  f1-score   support\n",
        "\n",
        "           0       0.70      0.19      0.30       288\n",
        "           1       0.95      0.99      0.97      4628\n",
        "\n",
        "    accuracy                           0.95      4916\n",
        "   macro avg       0.83      0.59      0.64      4916\n",
        "weighted avg       0.94      0.95      0.93      4916\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "############################\n",
        "# Optuna hyperparameter tuning\n",
        "###########################\n",
        "# REf: https://practicaldatascience.co.uk/machine-learning/how-to-use-optuna-for-xgboost-hyperparameter-tuning\n",
        "# Maximise f1_score.\n",
        "\n",
        "\n",
        "import optuna\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "\n",
        "# Get our train/test data:\n",
        "# Transformed data\n",
        "# Filter out initial 9\n",
        "#  cat columns. Keep only their\n",
        "#   numeric transformations\n",
        "\n",
        "l=train_trans.columns[9:]\n",
        "l\n",
        "tr_X =  train_trans[l] # Xtrain\n",
        "test_X = test_trans[l] # Xtest\n",
        "ytrain = y_train\n",
        "ytest = y_test\n",
        "\n",
        "# Original data\n",
        "tr_X =  X_train\n",
        "test_X = X_test\n",
        "ytrain = y_train\n",
        "ytest = y_test\n",
        "\n",
        "\n",
        "\n",
        "# Optuna, define objective function\n",
        "def objective(trial):\n",
        "    \"\"\"Define the objective function\"\"\"\n",
        "\n",
        "    # xgboost parameter ranges\n",
        "    params = {\n",
        "        'max_depth': trial.suggest_int('max_depth', 1, 14),\n",
        "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 1.0),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
        "        'gamma': trial.suggest_loguniform('gamma', 1e-8, 1.0),\n",
        "        'subsample': trial.suggest_loguniform('subsample', 0.01, 1.0),\n",
        "        'colsample_bytree': trial.suggest_loguniform('colsample_bytree', 0.01, 1.0),\n",
        "        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 1.0),\n",
        "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 1.0),\n",
        "        'eval_metric': 'auc',\n",
        "        'use_label_encoder': False\n",
        "    }\n",
        "\n",
        "\n",
        "    optuna_model = xgb.XGBClassifier(**params)\n",
        "    optuna_model.fit(tr_X, ytrain)\n",
        "    # Make predictions\n",
        "    y_pred = optuna_model.predict(test_X)\n",
        "\n",
        "    # Evaluate predictions\n",
        "    f1 = f1_score(ytest, y_pred, pos_label = 0)\n",
        "    # Maximise f1-score\n",
        "    return f1\n",
        "\n",
        "\n",
        "\n",
        "# Create optuna study\n",
        "study = optuna.create_study(direction='maximize')\n",
        "# Begin optimization\n",
        "study.optimize(objective, n_trials=400)\n",
        "# Can run this function again to optimize further\n",
        "study.optimize(objective, n_trials=200)\n",
        "\n",
        "\n",
        "# After study has finished:\n",
        "print('Number of finished trials: {}'.format(len(study.trials)))\n",
        "\n",
        "# Best trial\n",
        "trial = study.best_trial\n",
        "trial.value   # Best trial result (f1-score)\n",
        "# Get best parameters:\n",
        "for key, value in trial.params.items():\n",
        "    print('    {}: {}'.format(key, value))\n",
        "\n",
        "\n",
        "# Use these parameters in our estimator:\n",
        "best_params = trial.params\n",
        "\n",
        "model = xgb.XGBClassifier(**best_params)\n",
        "model.fit(tr_X, ytrain)\n",
        "\n",
        "# Make predictions and assessments:\n",
        "y_pred = model.predict(test_X)\n",
        "print(classification_report(ytest, y_pred))\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Results with transformed features (400 trials):\n",
        "==================================\n",
        "\n",
        "               precision    recall  f1-score   support\n",
        "\n",
        "           0       0.57      0.43      0.49       458\n",
        "           1       0.97      0.98      0.97      7735\n",
        "\n",
        "    accuracy                           0.95      8193\n",
        "   macro avg       0.77      0.70      0.73      8193\n",
        "weighted avg       0.94      0.95      0.95      8193\n",
        "\n",
        "\n",
        "\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.57      0.45      0.50       458\n",
        "           1       0.97      0.98      0.97      7735\n",
        "\n",
        "    accuracy                           0.95      8193\n",
        "   macro avg       0.77      0.71      0.74      8193\n",
        "weighted avg       0.95      0.95      0.95      8193\n",
        "\n",
        "\n",
        "\n",
        "Results with original data (400 trials):\n",
        "===========================\n",
        "\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "           0       0.65      0.40      0.49       458\n",
        "           1       0.97      0.99      0.98      7735\n",
        "\n",
        "    accuracy                           0.95      8193\n",
        "   macro avg       0.81      0.69      0.73      8193\n",
        "weighted avg       0.95      0.95      0.95      8193\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Also calculate 'auc':\n",
        "# If f1-score is maximised, there\n",
        "#  is some slight degradaion in 'auc'\n",
        "y_score = model.predict_proba(test_X)\n",
        "roc_auc_score(ytest, y_score[:,1])     # 0.846096/ 0.848417700973570\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "############################\n",
        "# Optuna hp tuning\n",
        "###########################\n",
        "# REf: https://practicaldatascience.co.uk/machine-learning/how-to-use-optuna-for-xgboost-hyperparameter-tuning\n",
        "# Maximise auc\n",
        "\n",
        "# Call libraries, as usuual:\n",
        "import optuna\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Train/test data\n",
        "# Filter out initial 9\n",
        "#  cat columns. Keep only their\n",
        "#   numeric transformations\n",
        "l=train_trans.columns[9:]\n",
        "l\n",
        "tr_X =  train_trans[l] # Xtrain\n",
        "test_X = test_trans[l] # Xtest\n",
        "ytrain = y_train\n",
        "ytest = y_test\n",
        "\n",
        "# Original data\n",
        "tr_X =  X_train\n",
        "test_X = X_test\n",
        "ytrain = y_train\n",
        "ytest = y_test\n",
        "\n",
        "\n",
        "def objective(trial):\n",
        "    \"\"\"Define the objective function\"\"\"\n",
        "\n",
        "   # Parameter ranges\n",
        "    params = {\n",
        "        'max_depth': trial.suggest_int('max_depth', 1, 14),\n",
        "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 1.0),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
        "        'gamma': trial.suggest_loguniform('gamma', 1e-8, 1.0),\n",
        "        'subsample': trial.suggest_loguniform('subsample', 0.01, 1.0),\n",
        "        'colsample_bytree': trial.suggest_loguniform('colsample_bytree', 0.01, 1.0),\n",
        "        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 1.0),\n",
        "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 1.0),\n",
        "        'eval_metric': 'auc',\n",
        "        'use_label_encoder': False\n",
        "    }\n",
        "\n",
        "\n",
        "    optuna_model = xgb.XGBClassifier(**params)\n",
        "    optuna_model.fit(tr_X, ytrain)\n",
        "    # Make predictions\n",
        "    y_score = optuna_model.predict_proba(test_X)\n",
        "    roc_score = roc_auc_score(ytest, y_score[:,1])\n",
        "\n",
        "    return roc_score\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=200)\n",
        "\n",
        "\n",
        "\n",
        "print('Number of finished trials: {}'.format(len(study.trials)))\n",
        "print('Best trial:')\n",
        "trial = study.best_trial\n",
        "\n",
        "print('  Value: {}'.format(trial.value))\n",
        "print('  Params: ')\n",
        "\n",
        "for key, value in trial.params.items():\n",
        "    print('    {}: {}'.format(key, value))\n",
        "\n",
        "\n",
        "params = trial.params\n",
        "\n",
        "model = xgb.XGBClassifier(**params)\n",
        "model.fit(tr_X, ytrain)\n",
        "\n",
        "\n",
        "y_score = model.predict_proba(test_X)\n",
        "roc_score = roc_auc_score(ytest, y_score[:,1])\n",
        "roc_score  #  0.8628952219114048\n",
        "\n",
        "y_pred = model.predict(test_X)\n",
        "print(classification_report(ytest, y_pred))\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "                        precision    recall  f1-score   support\n",
        "\n",
        "           0       0.60      0.28      0.38       458\n",
        "           1       0.96      0.99      0.97      7735\n",
        "\n",
        "    accuracy                           0.95      8193\n",
        "   macro avg       0.78      0.64      0.68      8193\n",
        "weighted avg       0.94      0.95      0.94      8193\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "########################################################\n",
        "# Optuna with SMOTE\n",
        "# Check if f1-score further improves\n",
        "########################################################\n",
        "\n",
        "\n",
        "import optuna\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Train/test data\n",
        "# Filter out initial 9\n",
        "#  cat columns. Keep only their\n",
        "#   numeric transformations\n",
        "l=train_trans.columns[9:]\n",
        "l\n",
        "tr_X =  train_trans[l] # Xtrain\n",
        "test_X = test_trans[l] # Xtest\n",
        "ytrain = y_train\n",
        "ytest = y_test\n",
        "\n",
        "\n",
        "sm = SMOTE(random_state=42)\n",
        "X_res, y_res = sm.fit_resample(tr_X, ytrain)\n",
        "X_res.shape  # (46274, 62)\n",
        "\n",
        "\n",
        "tr_X = X_res\n",
        "test_X = test_trans[l] # Xtest\n",
        "ytrain = y_res\n",
        "ytest = y_test\n",
        "\n",
        "\n",
        "# Optuna, define objective function\n",
        "def objective(trial):\n",
        "    \"\"\"Define the objective function\"\"\"\n",
        "\n",
        "    # xgboost parameter ranges\n",
        "    params = {\n",
        "        'max_depth': trial.suggest_int('max_depth', 1, 14),\n",
        "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 1.0),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
        "        'gamma': trial.suggest_loguniform('gamma', 1e-8, 1.0),\n",
        "        'subsample': trial.suggest_loguniform('subsample', 0.01, 1.0),\n",
        "        'colsample_bytree': trial.suggest_loguniform('colsample_bytree', 0.01, 1.0),\n",
        "        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 1.0),\n",
        "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 1.0),\n",
        "        'eval_metric': 'auc',\n",
        "        'use_label_encoder': False\n",
        "    }\n",
        "\n",
        "\n",
        "    optuna_model = xgb.XGBClassifier(**params)\n",
        "    optuna_model.fit(tr_X, ytrain)\n",
        "    # Make predictions\n",
        "    y_pred = optuna_model.predict(test_X)\n",
        "\n",
        "    # Evaluate predictions\n",
        "    f1 = f1_score(ytest, y_pred, pos_label = 0)\n",
        "    # Maximise f1-score\n",
        "    return f1\n",
        "\n",
        "\n",
        "\n",
        "# Create optuna study\n",
        "study = optuna.create_study(direction='maximize')\n",
        "# Begin optimization\n",
        "study.optimize(objective, n_trials=400)\n",
        "\n",
        "\n",
        "# After study has finished:\n",
        "print('Number of finished trials: {}'.format(len(study.trials)))\n",
        "\n",
        "# Best trial\n",
        "trial = study.best_trial\n",
        "trial.value   # Best trial,  0.5168986083499006\n",
        "# Get best parameters:\n",
        "for key, value in trial.params.items():\n",
        "    print('    {}: {}'.format(key, value))\n",
        "\n",
        "\n",
        "# Use these parameters in our estimator:\n",
        "best_params = trial.params\n",
        "\n",
        "model = xgb.XGBClassifier(**best_params)\n",
        "model.fit(tr_X, ytrain)\n",
        "\n",
        "# Make predictions and assessments:\n",
        "y_pred = model.predict(test_X)\n",
        "print(classification_report(ytest, y_pred))\n",
        "\n",
        "\"\"\"\n",
        "After using smote:\n",
        "\n",
        "                 precision    recall  f1-score   support\n",
        "\n",
        "           0       0.47      0.57      0.52       458\n",
        "           1       0.97      0.96      0.97      7735\n",
        "\n",
        "    accuracy                           0.94      8193\n",
        "   macro avg       0.72      0.77      0.74      8193\n",
        "weighted avg       0.95      0.94      0.94      8193\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "##*****************************\n",
        "## Embedding Projector\n",
        "## Incidentally f1-score is highest\n",
        "##*****************************\n",
        "\n",
        "os.chdir(master)\n",
        "train_trans = pd.read_pickle(\"X_train_transformed.pkl\")\n",
        "test_trans = pd.read_pickle(\"X_test_transformed.pkl\")\n",
        "X_train = pd.read_pickle(\"X_train.pkl\")\n",
        "X_test = pd.read_pickle(\"X_test.pkl\")\n",
        "y_train = pd.read_pickle(\"y_train.pkl\")\n",
        "y_test=pd.read_pickle(\"y_test.pkl\")\n",
        "\n",
        "train_trans.shape   #   (24576, 219)\n",
        "test_trans.shape    #  (8193, 219)\n",
        "train_trans.columns\n",
        "y_train.shape       # (24576,)\n",
        "y_test.shape        # (8193,)\n",
        "X_train.shape   # (24576, 9)\n",
        "#X_train['target'].head()  # Does it contain 'target'. No.\n",
        "\n",
        "\n",
        "# Impute test data\n",
        "train_trans.isnull().sum().sum()\n",
        "test_trans.isnull().sum().sum()   # 15796\n",
        "\n",
        "# Reset index\n",
        "y_train = y_train.reset_index(drop = True)\n",
        "y_test = y_test.reset_index(drop = True)\n",
        "\n",
        "test_trans['target'] = y_test\n",
        "train_trans['target'] =  y_train\n",
        "\n",
        "\n",
        "# Impute test data\n",
        "train_trans.isnull().sum().sum()\n",
        "test_trans.isnull().sum().sum()   # 15796\n",
        "\n",
        "\n",
        "test_trans = test_trans.dropna()\n",
        "test_trans = test_trans.reset_index(drop = True)\n",
        "test_trans.shape  #  (7740, 220)\n",
        "\n",
        "# Impute test data\n",
        "train_trans.isnull().sum().sum()\n",
        "test_trans.isnull().sum().sum()   # 15796\n",
        "\n",
        "train_trans.head()\n",
        "test_trans.head()\n",
        "test_trans.shape\n",
        "\n",
        "# Impute test_trans\n",
        "#si = SimpleImputer(strategy = 'mean')\n",
        "#si.fit(train_trans)\n",
        "#test_trans[:] = si.transform(test_trans)\n",
        "#test_trans.isnull().sum().sum()\n",
        "\n",
        "\n",
        "# Get embedding projector vectors and metadata\n",
        "# Needed to color\n",
        "train_trans.pop('resource')\n",
        "test_trans.pop('resource')\n",
        "\n",
        "yt_test = test_trans['target']\n",
        "yt_test.shape\n",
        "\n",
        "\n",
        "vec_tr = ct.vectorsToTSV(train_trans, take_mean = False, filepath = None, saveVectorsToDisk = True)\n",
        "vec_te = ct.vectorsToTSV(test_trans, take_mean = False, filepath = None, saveVectorsToDisk = False)\n",
        "\n",
        "\n",
        "\n",
        "cctr,ccte, vtr,vte = utils.pcaAndConcat(vec_tr, vec_te, n_components = 3)\n",
        "\n",
        "\n",
        "vtr.keys()\n",
        "vtr['mgrid'].head()\n",
        "\n",
        "cctr.shape   #  (24576, 16)\n",
        "ccte.shape   #  (8193, 16)\n",
        "\n",
        "cctr.columns\n",
        "ccte.columns\n",
        "\n",
        "Xtr_pca, Xte_pca, ytr_pca, yte_pca =  train_test_split(cctr, y_train, test_size = 0.20, stratify=y_train)\n",
        "\n",
        "model_vec= 0\n",
        "evals_result= {}\n",
        "del model_vec\n",
        "model_vec = xgb.XGBClassifier( n_estimators= 1000,\n",
        "                           verbosity = 3,\n",
        "                           eta = 0.04,      # 0.06\n",
        "                           max_depth = 13,\n",
        "                           subsample = 0.9,           # 0.8\n",
        "                           evals_result = evals_result,\n",
        "                           random_state = rng,\n",
        "                           reg_lambda = 1.5,\n",
        "\n",
        "\n",
        "                           )\n",
        "\n",
        "## NOTE THIS IS WITHOT resource column\n",
        "tr_X =    cctr # Xtr_pca # cctr #\n",
        "test_X =  ccte# Xte_pca # ccte #\n",
        "ytrain =  y_train # ytr_pca #  #  # y_train\n",
        "ytest = yt_test # yte_pca # yt_test # y_test #  #\n",
        "\n",
        "\n",
        "model_vec.fit(tr_X, ytrain.values,                   # Xtr, ytr\n",
        "          early_stopping_rounds = 100,   # 10% of n_estimators\n",
        "          eval_set=[ (test_X, ytest.values)],\n",
        "          eval_metric = ['auc']    # binary classification problem\n",
        "          )\n",
        "\n",
        "\n",
        "\n",
        "# auc: 0.81646\n",
        "model_vec.best_score   # 0.8622657  0.84430\n",
        "model_vec.best_iteration # 155\n",
        "pred = model_vec.predict(test_X)\n",
        "(pred == ytest).sum()/ytest.size    # 0.948858\n",
        "\n",
        "print(classification_report(ytest,pred))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##############################\n",
        "## tsne\n",
        "##############################\n",
        "# Why blobs do not appear together in tsne?\n",
        "# See StackOverflow:\n",
        "#    https://stats.stackexchange.com/a/453106/78454\n",
        "\n",
        "\n",
        "from sklearn.manifold import  TSNE\n",
        "\n",
        "\n",
        "## 2D\n",
        "tsne = TSNE()\n",
        "dx = tsne.fit_transform(orig_train)\n",
        "y_train.values.shape\n",
        "\n",
        "\n",
        "sns.scatterplot(x= dx[:,0], y = dx[:,1], hue = y_train.values)\n",
        "\n",
        "tsne = TSNE()\n",
        "org_trans_train.columns[20:]\n",
        "da = tsne.fit_transform(org_trans_train[org_trans_train.columns[20:]])\n",
        "da.shape\n",
        "sns.scatterplot(x= da[:,0], y = da[:,1], hue = y_train.values)\n",
        "sns.scatterplot(x= dx[:,0], y = dx[:,1], hue = y_train.values)\n",
        "\n",
        "\n",
        "## 3D\n",
        "tsne = TSNE(n_components = 3, early_exaggeration = 40)\n",
        "dx3 = tsne.fit_transform(orig_train)\n",
        "dx3.shape\n",
        "\n",
        "\n",
        "tsne = TSNE(n_components=3)\n",
        "org_trans_train.columns[20:]\n",
        "da3 = tsne.fit_transform(org_trans_train[org_trans_train.columns[20:]])\n",
        "da3.shape\n",
        "\n",
        "colnames = [\"c\" + str(i) for i in range(dx3.shape[1])]\n",
        "colnames\n",
        "dx3 = pd.DataFrame(dx3, columns = colnames)\n",
        "da3 = pd.DataFrame(da3, columns = colnames)\n",
        "\n",
        "dx3['target'] = y_train\n",
        "da3['target'] = y_train\n",
        "dx3.head()\n",
        "da3.head()\n",
        "\n",
        "os.chdir(master)\n",
        "dx3.to_csv(\"dx3.csv\", index = False)\n",
        "da3.to_csv(\"da3.csv\", index = False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train, X_test, ytrain, ytest = train_test_split(dx3.iloc[:,:3], y_train, test_size = 0.25 )\n",
        "Xtrain, Xtest, ytr, yte = train_test_split(da3.iloc[:,:3], y_train, test_size = 0.25 )\n",
        "\n",
        "evals_result= {}\n",
        "model_tsne = xgb.XGBClassifier( n_estimators= 1000,\n",
        "                           verbosity = 3,\n",
        "                           eta = 0.06,      # 0.06\n",
        "                           max_depth = 6,\n",
        "                           subsample = 0.8,           # 0.8\n",
        "                           evals_result = evals_result,\n",
        "                           random_state = 70\n",
        "                           )\n",
        "\n",
        "\n",
        "tr_X =  X_train\n",
        "test_X = X_test\n",
        "\n",
        "\n",
        "\n",
        "model_tsne.fit(tr_X, ytrain.values,                   # Xtr, ytr\n",
        "          early_stopping_rounds = 100,   # 10% of n_estimators\n",
        "          eval_set=[ (test_X, ytest.values)],\n",
        "          eval_metric = ['auc']\n",
        "          )\n",
        "\n",
        "\n",
        "\n",
        "# auc: 0.81646\n",
        "model_tsne.best_score   # 1.096898\n",
        "pred = model_tsne.predict(test_X)\n",
        "(pred == yte).sum()/yte.size    # 0.75\n",
        "\n",
        "\n",
        "\n",
        "##############################\n",
        "## umap\n",
        "##############################\n",
        "\n",
        "## 2D\n",
        "\n",
        "reducer = umap.UMAP()\n",
        "ss = StandardScaler()\n",
        "dx = reducer.fit_transform(ss.fit_transform(orig_train))\n",
        "\n",
        "sns.scatterplot(x= dx[:,0], y = dx[:,1], hue = y_train.values)\n",
        "\n",
        "reducer = umap.UMAP()\n",
        "ss = StandardScaler()\n",
        "org_trans_train.columns[20:]\n",
        "da = reducer.fit_transform(ss.fit_transform(org_trans_train[org_trans_train.columns[20:]]))\n",
        "da.shape\n",
        "sns.scatterplot(x= da[:,0], y = da[:,1], hue = y_train.values)\n",
        "sns.scatterplot(x= dx[:,0], y = dx[:,1], hue = y_train.values)\n",
        "\n",
        "\n",
        "colnames = [\"c\" + str(i) for i in range(dx.shape[1])]\n",
        "colnames\n",
        "dx = pd.DataFrame(dx, columns = colnames)\n",
        "da = pd.DataFrame(da, columns = colnames)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train, X_test, ytrain, ytest = train_test_split(dx, y_train, test_size = 0.25 )\n",
        "Xtrain, Xtest, ytr, yte = train_test_split(da, y_train, test_size = 0.25 )\n",
        "\n",
        "evals_result= {}\n",
        "model_umap = xgb.XGBClassifier( n_estimators= 1000,\n",
        "                           verbosity = 3,\n",
        "                           eta = 0.06,      # 0.06\n",
        "                           max_depth = 6,\n",
        "                           subsample = 0.8,           # 0.8\n",
        "                           evals_result = evals_result,\n",
        "                           random_state = 70\n",
        "                           )\n",
        "\n",
        "\n",
        "tr_X =  Xtrain\n",
        "test_X = Xtest\n",
        "\n",
        "\n",
        "\n",
        "model_umap.fit(tr_X, ytr.values,\n",
        "          early_stopping_rounds = 100,   # 10% of n_estimators\n",
        "          eval_set=[ (test_X, yte.values)],\n",
        "          eval_metric = ['auc']\n",
        "          )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model_umap.best_score\n",
        "pred = model_pca.predict(test_X)\n",
        "(pred == yte).sum()/yte.size\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#########################################\n",
        "## Predictive analytics\n",
        "########################################\n",
        "# Call it only once\n",
        "# See https://scikit-learn.org/stable/common_pitfalls.html#general-recommendations\n",
        "\n",
        "\n",
        "model0 = 0\n",
        "gc.collect()\n",
        "del model0\n",
        "evals_result= {}\n",
        "model0 = xgb.XGBClassifier( n_estimators= 1000,\n",
        "                           verbosity = 3,\n",
        "                           eta = 0.06,      # 0.06\n",
        "                           max_depth = 6,\n",
        "                           subsample = 0.8,           # 0.8\n",
        "                           evals_result = evals_result,\n",
        "                           random_state = seed\n",
        "                           )\n",
        "\n",
        "\n",
        "tr_X =  org_trans_train\n",
        "test_X =  org_trans_test\n",
        "\n",
        "\n",
        "\n",
        "model0.fit(tr_X, y_train.values,                   # Xtr, ytr\n",
        "          early_stopping_rounds = 100,   # 10% of n_estimators\n",
        "          eval_set=[ (test_X, y_test.values)],\n",
        "          eval_metric = ['auc']\n",
        "          )\n",
        "\n",
        "\n",
        "\n",
        "# auc: 0.81646\n",
        "model0.best_score   # 0.81761; 820858; 0.816837; 0.892089; 0.876738; 0.884359; 0.885373\n",
        "                    # 0.84595; 0.851114\n",
        "pred = model0.predict(test_X)\n",
        "(pred == y_test).sum()/y_test.size    # 0.7324 0.8022; 0.78395; 0.7954\n",
        "                                      # 0.7664;0.7716\n",
        "#plot_importance(model, importance_type = 'gain')\n",
        "\n",
        "\n",
        "\n",
        "fe_1, fe_0 = xg_impt_features(model0,org_trans_train.columns  )\n",
        "\n",
        "len(fe_1)   # 335  86  55 76   77  88\n",
        "len(fe_0)   # 743  11  11 14   16  16\n",
        "\n",
        "\n",
        "\n",
        "os.chdir(master)\n",
        "file = open('fe_1.txt','w')\n",
        "for  item in fe_1:\n",
        "\tfile.write(item+\"\\n\")\n",
        "file.close()\n",
        "\n",
        "# Read fe_1\n",
        "os.chdir(master)\n",
        "with open(\"fe_1.txt\", 'r') as f:\n",
        "    fe_1 = [line.rstrip('\\n') for line in f]\n",
        "\n",
        "len(fe_1)  # 77  88\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##---------------\n",
        "# With reduced best features\n",
        "model1 = 0\n",
        "gc.collect()\n",
        "del model1\n",
        "evals_result= {}\n",
        "model1 = xgb.XGBClassifier( n_estimators= 1000,\n",
        "                           verbosity = 3,\n",
        "                           eta = 0.06,      # 0.06\n",
        "                           max_depth = 6,\n",
        "                           subsample = 0.8,           # 0.8\n",
        "                           evals_result = evals_result,\n",
        "                           random_state = seed\n",
        "                           )\n",
        "\n",
        "\n",
        "tr_X =  org_trans_train[fe_1[:15]]     # Try from 7 to 30\n",
        "test_X =  org_trans_test[fe_1[:15]]\n",
        "\n",
        "\n",
        "\n",
        "model1.fit(tr_X, y_train.values,                   # Xtr, ytr\n",
        "          early_stopping_rounds = 100,   # 10% of n_estimators\n",
        "          eval_set=[ (test_X, y_test.values)],\n",
        "          eval_metric = ['auc']\n",
        "          )\n",
        "\n",
        "\n",
        "# auc: 0.81646\n",
        "model1.best_score   # 0.7228\n",
        "\n",
        "pred = model1.predict(test_X)\n",
        "(pred == y_test).sum()/y_test.size    # 0.5244\n",
        "\n",
        "\n",
        "fe_1[:6]\n",
        "\n",
        "\n",
        "fe_1[:7]\n",
        "\n",
        "##--------------------\n",
        "# orig + binned\n",
        "##--------------------\n",
        "gc.collect()\n",
        "#del model\n",
        "evals_result= {}\n",
        "model2 = xgb.XGBClassifier( n_estimators= 1000,\n",
        "                           verbosity = 3,\n",
        "                           eta = 0.06,      # 0.06\n",
        "                           max_depth = 6,\n",
        "                           subsample = 0.8,           # 0.8\n",
        "                           evals_result = evals_result,\n",
        "                           random_state = seed\n",
        "                           )\n",
        "\n",
        "\n",
        "tr_X =  org_binned_train\n",
        "test_X =  org_binned_test\n",
        "\n",
        "\n",
        "\n",
        "model2.fit(tr_X, y_train.values,                   # Xtr, ytr\n",
        "          early_stopping_rounds = 50,   # 10% of n_estimators\n",
        "          eval_set=[ (test_X, y_test.values)],\n",
        "          eval_metric = ['auc']\n",
        "          )\n",
        "\n",
        "\n",
        "\n",
        "# auc: 0.81646\n",
        "model2.best_score   # 0.821435 ; 827361 ; 0.897\n",
        "pred = model2.predict(test_X)\n",
        "(pred == y_test).sum()/y_test.size    # 0.7324 ; 0.81\n",
        "\n",
        "fe_11, fe_00 = xg_impt_features(model2,org_binned_train.columns  )\n",
        "len(fe_11)\n",
        "fe_00\n",
        "\n",
        "##-------------------\n",
        "# orig + binned best features\n",
        "##-------------------\n",
        "\n",
        "\n",
        "gc.collect()\n",
        "#del model\n",
        "evals_result= {}\n",
        "model3 = xgb.XGBClassifier( n_estimators= 1000,\n",
        "                           verbosity = 3,\n",
        "                           eta = 0.06,      # 0.06\n",
        "                           max_depth = 6,\n",
        "                           subsample = 0.8,           # 0.8\n",
        "                           evals_result = evals_result,\n",
        "                           random_state = rng\n",
        "                           )\n",
        "\n",
        "\n",
        "tr_X =  org_binned_train[fe_11]\n",
        "test_X =  org_binned_test[fe_11]\n",
        "\n",
        "\n",
        "\n",
        "model3.fit(tr_X, y_train.values,                   # Xtr, ytr\n",
        "          early_stopping_rounds = 50,   # 10% of n_estimators\n",
        "          eval_set=[ (test_X, y_test.values)],\n",
        "          eval_metric = ['auc']\n",
        "          )\n",
        "\n",
        "\n",
        "\n",
        "# auc: 0.81646\n",
        "model3.best_score   # 826236; 826423\n",
        "pred = model3.predict(test_X)\n",
        "(pred == y_test).sum()/y_test.size    # 0.7324\n",
        "\n",
        "\n",
        "\n",
        "##--------------------\n",
        "##-------------------\n",
        "# orig  features\n",
        "##-------------------\n",
        "\n",
        "\n",
        "model4 = 0\n",
        "\n",
        "gc.collect()\n",
        "del model4\n",
        "evals_result= {}\n",
        "model4 = xgb.XGBClassifier( n_estimators= 1000,\n",
        "                           verbosity = 3,\n",
        "                           eta = 0.06,      # 0.06\n",
        "                           max_depth = 6,\n",
        "                           subsample = 0.8,           # 0.8\n",
        "                           evals_result = evals_result,\n",
        "                           random_state = seed\n",
        "                           )\n",
        "\n",
        "\n",
        "tr_X =  orig_train[fe_4_1[:5]]\n",
        "test_X =  orig_test[fe_4_1[:5]]\n",
        "\n",
        "\n",
        "\n",
        "model4.fit(tr_X, y_train.values,                   # Xtr, ytr\n",
        "          early_stopping_rounds = 100,   # 10% of n_estimators\n",
        "          eval_set=[ (test_X, y_test.values)],\n",
        "          eval_metric = ['auc']\n",
        "          )\n",
        "\n",
        "\n",
        "\n",
        "# auc: 0.81646\n",
        "model4.best_score   # 0.7335065739582236\n",
        "pred = model4.predict(test_X)\n",
        "(pred == y_test).sum()/y_test.size    # 0.544\n",
        "\n",
        "fe_4_1, fe_4_0 = xg_impt_features(model4,orig_train.columns  )\n",
        "\n",
        "fe_4_1[:5]\n",
        "\n",
        "##--------------------\n",
        "\n",
        "fe_4_1[:5]\n",
        "\n",
        "model4_1 = 0\n",
        "\n",
        "gc.collect()\n",
        "del model4_1\n",
        "evals_result= {}\n",
        "model4_1 = xgb.XGBClassifier( n_estimators= 1000,\n",
        "                           verbosity = 3,\n",
        "                           eta = 0.06,      # 0.06\n",
        "                           max_depth = 6,\n",
        "                           subsample = 0.8,           # 0.8\n",
        "                           evals_result = evals_result,\n",
        "                           random_state = 70\n",
        "                           )\n",
        "\n",
        "\n",
        "tr_X =  orig_train[fe_4_1[:5]]\n",
        "test_X =  orig_test[fe_4_1[:5]]\n",
        "\n",
        "\n",
        "\n",
        "model4_1.fit(tr_X, y_train.values,                   # Xtr, ytr\n",
        "          early_stopping_rounds = 100,   # 10% of n_estimators\n",
        "          eval_set=[ (test_X, y_test.values)],\n",
        "          eval_metric = ['auc']\n",
        "          )\n",
        "\n",
        "\n",
        "\n",
        "# auc: 0.81646\n",
        "model4_1.best_score   # 831523 ; 824436 ; 0.8288 ; 0.897301 ; 0.880147; (0.891444, 0.892768, 0.893049)\n",
        "                    # (0.858484,0.862771, 0.874083 )\n",
        "pred = model4_1.predict(test_X)\n",
        "(pred == y_test).sum()/y_test.size    # 0.7376 ; 0.81; 0.7881; 0.8014, 0.8044\n",
        "                                      # 0.7788; 0.7918\n",
        "\n",
        "###################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###################################\n",
        "####################################\n",
        "\n",
        "\n",
        "y = train_train.pop('target')\n",
        "train_train.head()\n",
        "ohe = OneHotEncoder(  sparse = False)\n",
        "ohe.fit(train_train)\n",
        "train_ohe = ohe.transform(train_train)\n",
        "train_ohe.shape  # (7500, 89)\n",
        "cl = [\"c\" + str(i) for i in range(train_ohe.shape[1]) ]\n",
        "train_ohe = pd.DataFrame(train_ohe,columns = cl)\n",
        "train_ohe.head()\n",
        "train_ohe.shape  # (7500,75)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "pca = PCA(n_components=3)\n",
        "train_pca= pca.fit_transform(train_ohe)\n",
        "train_ohe.head()\n",
        "cx = [\"c\" + str(i) for i in range(train_pca.shape[1]) ]\n",
        "train_pca = pd.DataFrame(train_pca,columns = cx)\n",
        "train_pca.head()\n",
        "\n",
        "\n",
        "\n",
        "os.chdir(dataPath)\n",
        "\n",
        "train_pca.to_csv(\"train_pca.csv\", index = False)\n",
        "y.to_csv(\"y_train_pca.csv\", index = False)\n",
        "y.head()\n",
        "\n",
        "\n",
        "##################Model with orig data #####################\n",
        "\n",
        "\n",
        "X = orig_train\n",
        "y = orig_train.pop('target')\n",
        "X.columns\n",
        "X.head()\n",
        "y\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split( X,y,\n",
        "                                                 test_size = 0.25,\n",
        "                                                 stratify = y,\n",
        "                                                 random_state = 384)\n",
        "\n",
        "gc.collect()\n",
        "#del model\n",
        "evals_result= {}\n",
        "model = xgb.XGBClassifier( n_estimators= 700,\n",
        "                           verbosity = 3,\n",
        "                           eta = 0.06,      # 0.06\n",
        "                           max_depth = 6,\n",
        "                           subsample = 0.8,           # 0.8\n",
        "                           evals_result = evals_result,\n",
        "                           random_state = 800\n",
        "                           )\n",
        "\n",
        "tr_X =  X_train\n",
        "test_X =  X_test\n",
        "\n",
        "\n",
        "model.fit(tr_X, y_train,                   # Xtr, ytr\n",
        "          early_stopping_rounds = 50,   # 10% of n_estimators\n",
        "          eval_set=[ (test_X, y_test)],\n",
        "          eval_metric = ['merror']\n",
        "          )\n",
        "\n",
        "\n",
        "\n",
        "pred = model.predict(test_X)\n",
        "(pred == y_test).sum()/y_test.size    # 94.93%   91.8%  94.73  98.2(class_Sep = 2.0)\n",
        "plot_importance(model, importance_type = 'gain')\n",
        "\n",
        "################## Model with discrete features #####################\n",
        "\n",
        "\n",
        "X = train_train\n",
        "y = train_train.pop('target')\n",
        "X.columns\n",
        "X.head()\n",
        "y\n",
        "\n",
        "for i,j in enumerate(X.columns):\n",
        "    X[j] = X[j].astype('int')\n",
        "\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split( X,y,\n",
        "                                                 test_size = 0.25,\n",
        "                                                 stratify = y,\n",
        "                                                 random_state = 384)\n",
        "\n",
        "gc.collect()\n",
        "del model\n",
        "evals_result= {}\n",
        "model = xgb.XGBClassifier( n_estimators= 700,\n",
        "                           verbosity = 3,\n",
        "                           eta = 0.06,      # 0.06\n",
        "                           max_depth = 6,\n",
        "                           subsample = 0.8,           # 0.8\n",
        "                           evals_result = evals_result,\n",
        "                           random_state = 800\n",
        "                           )\n",
        "\n",
        "tr_X =  X_train\n",
        "test_X =  X_test\n",
        "\n",
        "\n",
        "model.fit(tr_X, y_train,                   # Xtr, ytr\n",
        "          early_stopping_rounds = 50,   # 10% of n_estimators\n",
        "          eval_set=[ (test_X, y_test)],\n",
        "          eval_metric = ['merror']\n",
        "          )\n",
        "\n",
        "\n",
        "\n",
        "pred = model.predict(test_X)\n",
        "(pred == y_test).sum()/y_test.size    # 94.6% ; 95%  90.8%  94.86  98.86(class sep = 2.0)\n",
        "plot_importance(model, importance_type = 'gain')\n",
        "\n",
        "##############################################################\n",
        "\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "fig,ax= plt.subplots(1,1,figsize = (10,10))\n",
        "sns.scatterplot(data = tr_X, x = 'fe', y = 'fd', hue= y_train, ax = ax, alpha = 0.4)\n",
        "\n",
        "fig,ax= plt.subplots(1,1,figsize = (10,10))\n",
        "sns.scatterplot(data = orig_train, x = 'fe', y = 'fb', hue= y,ax=ax ,palette = \"Set2\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#################################################################\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(1)\n",
        "plt.clf()\n",
        "colors = [\"#dede00\", \"#377eb8\", \"#f781bf\"]\n",
        "markers = [\"x\", \"o\", \"^\"]\n",
        "\n",
        "# Three clusters can be seen\n",
        "fig = plt.figure(figsize = (8,8))\n",
        "_=sns.scatterplot(data = X, x = \"x1\", y = \"x2\", hue = y)\n",
        "\n",
        "fig = plt.figure(figsize = (8,8))\n",
        "_=sns.scatterplot(data = X, x = \"x2\", y = \"x3\", hue = y)\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize = (8,8)) ;\n",
        "_=sns.scatterplot(data = X, x = \"x1\", y = \"x3\", hue = y)"
      ]
    }
  ]
}